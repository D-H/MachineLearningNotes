{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "\n",
    "__Information entropy__ is the average rate at which information is produced by a stochastic (random) source of data. \n",
    "\n",
    "The measure of entropy associated with each possible data value is the negative logarithm of the probability mass function for the value. Thus, when the data source has a low probability value the event carries more \"information\" (\"surprisal\") then when the source data has a high-probability value. \n",
    "\n",
    "In statistics, the __probability mass function__ is a function that gives the probability that some discrete variable is equal to some exact value. The probability mass function differs from the __probability density function__ in that the latter is a function that is used for continuous variables and not discrete values. \n",
    "\n",
    "Information entropy is typically measured in __bits__. \n",
    "\n",
    "The logarithm of the probability distribution is useful as a measure of entropy because it is additive for independent sources. For instance, the entropy of a fair coin toss is 1 bit, and the entropy of m tosses is m bits. \n",
    "\n",
    "For example, $log_2(n)$ bits are needed to represent a variable that can take one of n values.\n",
    "\n",
    "The basic idea of information theory is the more one knows about a topic, the less new information one is apt to get about it. If an event is very probable, it is no surprise when it happens and thus provides little new information. Inversely, if the event was improbable, it is much more informative that the event happened. Therefore, the information content is an increasing function of the inverse of the probability of the event (1/p). Now, if more events may happen, entropy measures the average information content you can expect to get if one of the events actually happens. This implies that casting a die has more entropy than tossing a coin because each outcome of the die has smaller probability than each outcome of the coin. Thus, entropy is a measure of unpredictability of the state, or equivalently, of its average information content. \n",
    "\n",
    "Entropy can be written as $H(X) = -\\sum_{i=1}^{n}P(x_i)log_bP(x_i)$ where b is the base of the logarithm used. \n",
    "\n",
    "Entropy is maximized when you are less sure what the outcome is going to be. Just look at the example where you have a coin, let's say you have a fair coin so the odds of heads is 0.5 and tails is 0.5. Then your entropy will be equal to \n",
    "\n",
    "$H(X) = -\\sum_{i=1}^{2}(0.5)log_2(0.5) = 1$ \n",
    "\n",
    "But now let's say you have a coin that is unfair, so that the odds of the value being heads is 0.999 and the value of tails is 0.001. Your entropy would be equal to. \n",
    "\n",
    "$H(X) = -(0.999)log_2(0.999) - (0.001)log_2(0.001)  = 0.0114$\n",
    "\n",
    "__Formula for the entropy of a discrete random variable__ : $H(X) = -log(P(X))$ where X is the discrete value and P(X) and the value for that discrete value in the probability mass function.\n",
    "\n",
    "This makes me believe that entropy is the average number of bits that we need to send to convey the data that we want. The higher the number of bits we need on average the higher the entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example taken from article. \n",
    "\n",
    "Let's say you're standing next to a highway in Boston during rush hour, watching cars inch by, and you'd like to communicate each car model you see to a friend. You're stuck with a binary channel through which you can send *0* or *1*, and it's expensive: you're charged `$0.10` per bit.\n",
    "\n",
    "You need many bit sequences, one for each car model.\n",
    "\n",
    "How would you allocate bit sequences to the car models? Would you use the same number of bits for the Toyota Camry sequence as you would for the Tesla Model S sequence?\n",
    "\n",
    "No, you wouldn't. You'd allocate less bits to the Camry, because you know that you'll end up sending the Camry sequence much more often than the Tesla Model S sequence. What you're doing is exploiting your knowledge about the distribution over car models to reduce the number of bits that you need to send on average.\n",
    "\n",
    "It turns out that if you have access to the underlying distribution $y$, then to use the smallest number of bits on average, you should assign $\\log \\frac{1}{y_i}$ bits to the $i$-th symbol. (As a reminder, $y_i$ is the probability of the $i$-th symbol.)\n",
    "\n",
    "For example, if we assume that seeing a Toyota is 128x as likely as seeing a Tesla, then we'd give the Toyota symbol 7 less bits than the Tesla symbol:\n",
    "\n",
    "$$ b_\\mbox{toyota}\n",
    "= \\log \\frac{1}{128 p_\\mbox{tesla}}\n",
    "= \\log \\frac{1}{p_\\mbox{tesla}} + \\log \\frac{1}{128}\n",
    "= b_\\mbox{tesla} - 7 $$\n",
    "\n",
    "By fully exploiting the known distribution $y$ in this way, we achieve an optimal number of bits per transmission. The optimal number of bits is known as [entropy](https://en.wikipedia.org/wiki/Entropy_%28information_theory%29). Mathematically, it's just the expected number of bits under this optimal encoding:\n",
    "\n",
    "$$ H(y) = \\sum_i y_i \\log \\frac{1}{y_i} = -\\sum_i y_i \\log y_i $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
