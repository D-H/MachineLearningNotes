{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhenderson\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IZYv70SvvOan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-3b42a02b2811>:45: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [],
   "source": [
    "# num_steps = 1001\n",
    "\n",
    "# with tf.Session(graph=graph) as session:\n",
    "#   tf.global_variables_initializer().run()\n",
    "#   print('Initialized')\n",
    "#   for step in range(num_steps):\n",
    "#     offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "#     batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "#     batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "#     feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "#     _, l, predictions = session.run(\n",
    "#       [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "#     if (step % 50 == 0):\n",
    "#       print('Minibatch loss at step %d: %f' % (step, l))\n",
    "#       print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "#       print('Validation accuracy: %.1f%%' % accuracy(\n",
    "#         valid_prediction.eval(), valid_labels))\n",
    "#   print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 28, 28, 25)\n",
      "(16, 14, 14, 25)\n",
      "(16, 7, 7, 25)\n",
      "(10000, 28, 28, 25)\n",
      "(10000, 14, 14, 25)\n",
      "(10000, 7, 7, 25)\n",
      "(10000, 28, 28, 25)\n",
      "(10000, 14, 14, 25)\n",
      "(10000, 7, 7, 25)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 25\n",
    "num_hidden = 64\n",
    "num_channels = 1\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data, train=False): \n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    print(conv.get_shape())\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    pooled = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"SAME\")\n",
    "    print(pooled.get_shape())\n",
    "    conv = tf.nn.conv2d(pooled, layer2_weights, [1, 1, 1, 1], padding=\"SAME\")\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    pooled = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"SAME\")\n",
    "    print(pooled.get_shape())\n",
    "    shape = pooled.get_shape().as_list()\n",
    "    reshape = tf.reshape(pooled, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    tf.matmul(reshape, layer3_weights)\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    \n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5)\n",
    "        \n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, True)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "  # adding regularizers\n",
    "  regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) +\n",
    "                  tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) +\n",
    "                  tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) +\n",
    "                  tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer4_biases)\n",
    "                 )\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 3e-4 * regularizers\n",
    "    \n",
    "  # Optimizer: set up a variable that's incremented once per batch and\n",
    "  # controls the learning rate decay.\n",
    "  batch = tf.Variable(0)\n",
    "  # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "      0.01,                # Base learning rate.\n",
    "      batch * batch_size,  # Current index into the dataset.\n",
    "      train_labels.shape[0],          # Decay step.\n",
    "      0.95,                # Decay rate.\n",
    "      staircase=True)\n",
    "  # Use simple momentum for the optimization.\n",
    "  optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                         0.9).minimize(loss,\n",
    "                                                       global_step=batch)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 6.097877\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.7%\n",
      "Minibatch loss at step 50: 2.249521\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 27.0%\n",
      "Minibatch loss at step 100: 2.272937\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 20.9%\n",
      "Minibatch loss at step 150: 2.127567\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 26.5%\n",
      "Minibatch loss at step 200: 1.814353\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 250: 1.801741\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 69.4%\n",
      "Minibatch loss at step 300: 1.705539\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 350: 1.253408\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 400: 1.145070\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 450: 0.960605\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 500: 1.549298\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 550: 0.900620\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 600: 0.873903\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 650: 0.983971\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 700: 0.913950\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 750: 0.485921\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 800: 0.853831\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 850: 1.471626\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 900: 0.741423\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 950: 0.794868\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 1000: 0.766339\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 1050: 1.072163\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1100: 1.060062\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1150: 0.828845\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 1200: 1.282058\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 1250: 0.788335\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 1300: 0.411711\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 1350: 0.940521\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 1400: 0.394446\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 1450: 0.660533\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1500: 0.960492\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1550: 0.965507\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1600: 0.977669\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 1650: 0.971346\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1700: 0.750888\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 1750: 0.698502\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 1800: 0.617983\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 1850: 1.197051\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1900: 0.472194\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 1950: 0.745200\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 2000: 0.409337\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 2050: 1.136652\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 2100: 0.495228\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 2150: 0.659727\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2200: 0.507828\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 2250: 0.771012\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 2300: 0.696188\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 2350: 0.756866\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 2400: 1.077554\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 2450: 0.659709\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2500: 0.910386\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 2550: 0.976469\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 2600: 0.728986\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 2650: 0.652078\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 2700: 0.433315\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 2750: 1.018761\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 2800: 0.598841\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 2850: 0.334817\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 2900: 0.611585\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 2950: 0.466308\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 3000: 1.244637\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.5%\n",
      "Test accuracy: 92.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---\n",
    "\n",
    "For reference I did not come up with this, I have copied this from this solution I found online. \n",
    "\n",
    "https://github.com/sjuvekar/Udacity-Deep-Learning/blob/master/L3/4_convolutions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "Step 0 (epoch 0.00), 4.3 ms\n",
      "Minibatch loss: 9.657, learning rate: 0.010000\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 18.9%\n",
      "Step 100 (epoch 0.03), 150.5 ms\n",
      "Minibatch loss: 3.617, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.2%\n",
      "Step 200 (epoch 0.06), 149.9 ms\n",
      "Minibatch loss: 3.594, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.0%\n",
      "Step 300 (epoch 0.10), 148.6 ms\n",
      "Minibatch loss: 3.852, learning rate: 0.010000\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 83.9%\n",
      "Step 400 (epoch 0.13), 148.1 ms\n",
      "Minibatch loss: 3.937, learning rate: 0.010000\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 85.1%\n",
      "Step 500 (epoch 0.16), 148.1 ms\n",
      "Minibatch loss: 3.345, learning rate: 0.010000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 85.5%\n",
      "Step 600 (epoch 0.19), 148.3 ms\n",
      "Minibatch loss: 3.478, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.2%\n",
      "Step 700 (epoch 0.22), 150.7 ms\n",
      "Minibatch loss: 3.326, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.2%\n",
      "Step 800 (epoch 0.26), 153.9 ms\n",
      "Minibatch loss: 3.348, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.3%\n",
      "Step 900 (epoch 0.29), 148.8 ms\n",
      "Minibatch loss: 3.367, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.0%\n",
      "Step 1000 (epoch 0.32), 148.7 ms\n",
      "Minibatch loss: 3.289, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.2%\n",
      "Step 1100 (epoch 0.35), 157.2 ms\n",
      "Minibatch loss: 3.561, learning rate: 0.010000\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 87.3%\n",
      "Step 1200 (epoch 0.38), 151.0 ms\n",
      "Minibatch loss: 3.101, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Step 1300 (epoch 0.42), 153.1 ms\n",
      "Minibatch loss: 3.036, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.3%\n",
      "Step 1400 (epoch 0.45), 158.6 ms\n",
      "Minibatch loss: 3.287, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.6%\n",
      "Step 1500 (epoch 0.48), 158.5 ms\n",
      "Minibatch loss: 3.146, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.8%\n",
      "Step 1600 (epoch 0.51), 152.1 ms\n",
      "Minibatch loss: 2.852, learning rate: 0.010000\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 87.7%\n",
      "Step 1700 (epoch 0.54), 152.3 ms\n",
      "Minibatch loss: 3.104, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Step 1800 (epoch 0.58), 155.1 ms\n",
      "Minibatch loss: 2.984, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.1%\n",
      "Step 1900 (epoch 0.61), 151.8 ms\n",
      "Minibatch loss: 3.019, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.2%\n",
      "Step 2000 (epoch 0.64), 156.2 ms\n",
      "Minibatch loss: 2.893, learning rate: 0.010000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.3%\n",
      "Step 2100 (epoch 0.67), 151.6 ms\n",
      "Minibatch loss: 2.740, learning rate: 0.010000\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 88.2%\n",
      "Step 2200 (epoch 0.70), 148.2 ms\n",
      "Minibatch loss: 2.945, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.7%\n",
      "Step 2300 (epoch 0.74), 148.4 ms\n",
      "Minibatch loss: 3.114, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.7%\n",
      "Step 2400 (epoch 0.77), 149.6 ms\n",
      "Minibatch loss: 2.870, learning rate: 0.010000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.8%\n",
      "Step 2500 (epoch 0.80), 150.6 ms\n",
      "Minibatch loss: 2.778, learning rate: 0.010000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.0%\n",
      "Step 2600 (epoch 0.83), 154.2 ms\n",
      "Minibatch loss: 2.673, learning rate: 0.010000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.9%\n",
      "Step 2700 (epoch 0.86), 151.3 ms\n",
      "Minibatch loss: 2.943, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.2%\n",
      "Step 2800 (epoch 0.90), 152.6 ms\n",
      "Minibatch loss: 2.786, learning rate: 0.010000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Step 2900 (epoch 0.93), 148.2 ms\n",
      "Minibatch loss: 2.794, learning rate: 0.010000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.0%\n",
      "Step 3000 (epoch 0.96), 148.9 ms\n",
      "Minibatch loss: 2.717, learning rate: 0.010000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.8%\n",
      "Step 3100 (epoch 0.99), 149.2 ms\n",
      "Minibatch loss: 2.776, learning rate: 0.010000\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.1%\n",
      "Step 3200 (epoch 1.02), 148.3 ms\n",
      "Minibatch loss: 2.558, learning rate: 0.009500\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.1%\n",
      "Step 3300 (epoch 1.06), 153.0 ms\n",
      "Minibatch loss: 2.601, learning rate: 0.009500\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.4%\n",
      "Step 3400 (epoch 1.09), 148.6 ms\n",
      "Minibatch loss: 2.374, learning rate: 0.009500\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.3%\n",
      "Step 3500 (epoch 1.12), 150.5 ms\n",
      "Minibatch loss: 2.483, learning rate: 0.009500\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.3%\n",
      "Step 3600 (epoch 1.15), 148.2 ms\n",
      "Minibatch loss: 2.645, learning rate: 0.009500\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.5%\n",
      "Step 3700 (epoch 1.18), 152.3 ms\n",
      "Minibatch loss: 2.420, learning rate: 0.009500\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 89.4%\n",
      "Step 3800 (epoch 1.22), 148.2 ms\n",
      "Minibatch loss: 2.740, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.1%\n",
      "Step 3900 (epoch 1.25), 150.9 ms\n",
      "Minibatch loss: 2.345, learning rate: 0.009500\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Step 4000 (epoch 1.28), 149.4 ms\n",
      "Minibatch loss: 2.517, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4100 (epoch 1.31), 150.3 ms\n",
      "Minibatch loss: 2.586, learning rate: 0.009500\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4200 (epoch 1.34), 149.3 ms\n",
      "Minibatch loss: 2.599, learning rate: 0.009500\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.6%\n",
      "Step 4300 (epoch 1.38), 149.5 ms\n",
      "Minibatch loss: 2.348, learning rate: 0.009500\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.8%\n",
      "Step 4400 (epoch 1.41), 150.1 ms\n",
      "Minibatch loss: 2.289, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.7%\n",
      "Step 4500 (epoch 1.44), 148.9 ms\n",
      "Minibatch loss: 2.177, learning rate: 0.009500\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.8%\n",
      "Step 4600 (epoch 1.47), 149.6 ms\n",
      "Minibatch loss: 2.562, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.6%\n",
      "Step 4700 (epoch 1.50), 149.8 ms\n",
      "Minibatch loss: 2.436, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.8%\n",
      "Step 4800 (epoch 1.54), 148.7 ms\n",
      "Minibatch loss: 2.425, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.8%\n",
      "Step 4900 (epoch 1.57), 148.2 ms\n",
      "Minibatch loss: 2.112, learning rate: 0.009500\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.8%\n",
      "Step 5000 (epoch 1.60), 151.4 ms\n",
      "Minibatch loss: 2.313, learning rate: 0.009500\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.8%\n",
      "Step 5100 (epoch 1.63), 151.5 ms\n",
      "Minibatch loss: 2.506, learning rate: 0.009500\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.8%\n",
      "Step 5200 (epoch 1.66), 148.4 ms\n",
      "Minibatch loss: 2.406, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.1%\n",
      "Step 5300 (epoch 1.70), 150.1 ms\n",
      "Minibatch loss: 2.295, learning rate: 0.009500\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.1%\n",
      "Step 5400 (epoch 1.73), 148.2 ms\n",
      "Minibatch loss: 2.254, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.1%\n",
      "Step 5500 (epoch 1.76), 150.2 ms\n",
      "Minibatch loss: 2.144, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.2%\n",
      "Step 5600 (epoch 1.79), 148.7 ms\n",
      "Minibatch loss: 2.118, learning rate: 0.009500\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.3%\n",
      "Step 5700 (epoch 1.82), 148.3 ms\n",
      "Minibatch loss: 2.144, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Step 5800 (epoch 1.86), 148.2 ms\n",
      "Minibatch loss: 2.224, learning rate: 0.009500\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.2%\n",
      "Step 5900 (epoch 1.89), 149.7 ms\n",
      "Minibatch loss: 2.076, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.7%\n",
      "Step 6000 (epoch 1.92), 148.9 ms\n",
      "Minibatch loss: 1.913, learning rate: 0.009500\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.2%\n",
      "Step 6100 (epoch 1.95), 148.4 ms\n",
      "Minibatch loss: 2.037, learning rate: 0.009500\n",
      "Minibatch accuracy: 87.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 90.4%\n",
      "Step 6200 (epoch 1.98), 149.0 ms\n",
      "Minibatch loss: 2.135, learning rate: 0.009500\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Step 6300 (epoch 2.02), 148.2 ms\n",
      "Minibatch loss: 2.128, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.0%\n",
      "Step 6400 (epoch 2.05), 148.3 ms\n",
      "Minibatch loss: 2.078, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.0%\n",
      "Step 6500 (epoch 2.08), 153.2 ms\n",
      "Minibatch loss: 1.843, learning rate: 0.009025\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Step 6600 (epoch 2.11), 151.9 ms\n",
      "Minibatch loss: 1.895, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Step 6700 (epoch 2.14), 149.6 ms\n",
      "Minibatch loss: 1.985, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.5%\n",
      "Step 6800 (epoch 2.18), 149.7 ms\n",
      "Minibatch loss: 1.768, learning rate: 0.009025\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.2%\n",
      "Step 6900 (epoch 2.21), 149.9 ms\n",
      "Minibatch loss: 1.888, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.4%\n",
      "Step 7000 (epoch 2.24), 148.2 ms\n",
      "Minibatch loss: 1.861, learning rate: 0.009025\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Step 7100 (epoch 2.27), 150.4 ms\n",
      "Minibatch loss: 1.932, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.5%\n",
      "Step 7200 (epoch 2.30), 149.1 ms\n",
      "Minibatch loss: 1.995, learning rate: 0.009025\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.4%\n",
      "Step 7300 (epoch 2.34), 149.5 ms\n",
      "Minibatch loss: 1.952, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.7%\n",
      "Step 7400 (epoch 2.37), 148.7 ms\n",
      "Minibatch loss: 2.160, learning rate: 0.009025\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 90.3%\n",
      "Step 7500 (epoch 2.40), 149.4 ms\n",
      "Minibatch loss: 1.949, learning rate: 0.009025\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 90.6%\n",
      "Step 7600 (epoch 2.43), 148.9 ms\n",
      "Minibatch loss: 1.833, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.6%\n",
      "Step 7700 (epoch 2.46), 149.5 ms\n",
      "Minibatch loss: 1.744, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.6%\n",
      "Step 7800 (epoch 2.50), 148.9 ms\n",
      "Minibatch loss: 1.738, learning rate: 0.009025\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.4%\n",
      "Step 7900 (epoch 2.53), 149.1 ms\n",
      "Minibatch loss: 1.769, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.4%\n",
      "Step 8000 (epoch 2.56), 150.8 ms\n",
      "Minibatch loss: 1.795, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.5%\n",
      "Step 8100 (epoch 2.59), 150.6 ms\n",
      "Minibatch loss: 1.732, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.6%\n",
      "Step 8200 (epoch 2.62), 153.4 ms\n",
      "Minibatch loss: 1.672, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Step 8300 (epoch 2.66), 151.2 ms\n",
      "Minibatch loss: 1.650, learning rate: 0.009025\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.7%\n",
      "Step 8400 (epoch 2.69), 148.8 ms\n",
      "Minibatch loss: 1.953, learning rate: 0.009025\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 90.8%\n",
      "Step 8500 (epoch 2.72), 149.1 ms\n",
      "Minibatch loss: 1.910, learning rate: 0.009025\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 90.6%\n",
      "Step 8600 (epoch 2.75), 148.8 ms\n",
      "Minibatch loss: 1.655, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.7%\n",
      "Step 8700 (epoch 2.78), 148.7 ms\n",
      "Minibatch loss: 1.831, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.4%\n",
      "Step 8800 (epoch 2.82), 150.4 ms\n",
      "Minibatch loss: 1.662, learning rate: 0.009025\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.8%\n",
      "Step 8900 (epoch 2.85), 150.3 ms\n",
      "Minibatch loss: 1.617, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.7%\n",
      "Step 9000 (epoch 2.88), 160.2 ms\n",
      "Minibatch loss: 1.544, learning rate: 0.009025\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Step 9100 (epoch 2.91), 154.8 ms\n",
      "Minibatch loss: 1.707, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Step 9200 (epoch 2.94), 157.1 ms\n",
      "Minibatch loss: 1.578, learning rate: 0.009025\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Step 9300 (epoch 2.98), 158.6 ms\n",
      "Minibatch loss: 1.650, learning rate: 0.009025\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.8%\n",
      "Step 9400 (epoch 3.01), 151.3 ms\n",
      "Minibatch loss: 1.622, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.7%\n",
      "Step 9500 (epoch 3.04), 153.9 ms\n",
      "Minibatch loss: 1.621, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.5%\n",
      "Step 9600 (epoch 3.07), 161.3 ms\n",
      "Minibatch loss: 1.563, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Step 9700 (epoch 3.10), 169.1 ms\n",
      "Minibatch loss: 1.430, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.7%\n",
      "Step 9800 (epoch 3.14), 151.5 ms\n",
      "Minibatch loss: 1.562, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.0%\n",
      "Step 9900 (epoch 3.17), 148.1 ms\n",
      "Minibatch loss: 1.544, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.9%\n",
      "Step 10000 (epoch 3.20), 148.1 ms\n",
      "Minibatch loss: 1.408, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.9%\n",
      "Step 10100 (epoch 3.23), 149.2 ms\n",
      "Minibatch loss: 1.498, learning rate: 0.008574\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.9%\n",
      "Step 10200 (epoch 3.26), 149.3 ms\n",
      "Minibatch loss: 1.366, learning rate: 0.008574\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.2%\n",
      "Step 10300 (epoch 3.30), 149.3 ms\n",
      "Minibatch loss: 1.492, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Step 10400 (epoch 3.33), 148.6 ms\n",
      "Minibatch loss: 1.476, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.9%\n",
      "Step 10500 (epoch 3.36), 148.6 ms\n",
      "Minibatch loss: 1.587, learning rate: 0.008574\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.0%\n",
      "Step 10600 (epoch 3.39), 148.3 ms\n",
      "Minibatch loss: 1.362, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.1%\n",
      "Step 10700 (epoch 3.42), 150.0 ms\n",
      "Minibatch loss: 1.431, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.1%\n",
      "Step 10800 (epoch 3.46), 148.2 ms\n",
      "Minibatch loss: 1.658, learning rate: 0.008574\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.2%\n",
      "Step 10900 (epoch 3.49), 149.5 ms\n",
      "Minibatch loss: 1.451, learning rate: 0.008574\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.1%\n",
      "Step 11000 (epoch 3.52), 148.2 ms\n",
      "Minibatch loss: 1.458, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.0%\n",
      "Step 11100 (epoch 3.55), 150.0 ms\n",
      "Minibatch loss: 1.375, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.2%\n",
      "Step 11200 (epoch 3.58), 148.0 ms\n",
      "Minibatch loss: 1.387, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.1%\n",
      "Step 11300 (epoch 3.62), 148.2 ms\n",
      "Minibatch loss: 1.398, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.2%\n",
      "Step 11400 (epoch 3.65), 151.1 ms\n",
      "Minibatch loss: 1.261, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.1%\n",
      "Step 11500 (epoch 3.68), 149.0 ms\n",
      "Minibatch loss: 1.413, learning rate: 0.008574\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.1%\n",
      "Step 11600 (epoch 3.71), 148.8 ms\n",
      "Minibatch loss: 1.653, learning rate: 0.008574\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.2%\n",
      "Step 11700 (epoch 3.74), 148.4 ms\n",
      "Minibatch loss: 1.363, learning rate: 0.008574\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.0%\n",
      "Step 11800 (epoch 3.78), 148.0 ms\n",
      "Minibatch loss: 1.328, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.3%\n",
      "Step 11900 (epoch 3.81), 148.7 ms\n",
      "Minibatch loss: 1.319, learning rate: 0.008574\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.2%\n",
      "Step 12000 (epoch 3.84), 149.3 ms\n",
      "Minibatch loss: 1.343, learning rate: 0.008574\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.3%\n",
      "Step 12100 (epoch 3.87), 149.8 ms\n",
      "Minibatch loss: 1.186, learning rate: 0.008574\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.3%\n",
      "Step 12200 (epoch 3.90), 148.4 ms\n",
      "Minibatch loss: 1.248, learning rate: 0.008574\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.3%\n",
      "Step 12300 (epoch 3.94), 148.7 ms\n",
      "Minibatch loss: 1.175, learning rate: 0.008574\n",
      "Minibatch accuracy: 93.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 91.2%\n",
      "Step 12400 (epoch 3.97), 148.2 ms\n",
      "Minibatch loss: 1.341, learning rate: 0.008574\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.4%\n",
      "Step 12500 (epoch 4.00), 149.1 ms\n",
      "Minibatch loss: 1.112, learning rate: 0.008145\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.1%\n",
      "Step 12600 (epoch 4.03), 148.4 ms\n",
      "Minibatch loss: 1.344, learning rate: 0.008145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.2%\n",
      "Step 12700 (epoch 4.06), 150.2 ms\n",
      "Minibatch loss: 1.243, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.3%\n",
      "Step 12800 (epoch 4.10), 148.6 ms\n",
      "Minibatch loss: 1.363, learning rate: 0.008145\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.3%\n",
      "Step 12900 (epoch 4.13), 149.8 ms\n",
      "Minibatch loss: 1.081, learning rate: 0.008145\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.2%\n",
      "Step 13000 (epoch 4.16), 153.1 ms\n",
      "Minibatch loss: 1.375, learning rate: 0.008145\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Step 13100 (epoch 4.19), 156.4 ms\n",
      "Minibatch loss: 1.236, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.3%\n",
      "Step 13200 (epoch 4.22), 160.8 ms\n",
      "Minibatch loss: 1.295, learning rate: 0.008145\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.2%\n",
      "Step 13300 (epoch 4.26), 157.3 ms\n",
      "Minibatch loss: 1.077, learning rate: 0.008145\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.2%\n",
      "Step 13400 (epoch 4.29), 155.9 ms\n",
      "Minibatch loss: 1.050, learning rate: 0.008145\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13500 (epoch 4.32), 155.4 ms\n",
      "Minibatch loss: 1.085, learning rate: 0.008145\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.3%\n",
      "Step 13600 (epoch 4.35), 155.8 ms\n",
      "Minibatch loss: 1.269, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13700 (epoch 4.38), 154.3 ms\n",
      "Minibatch loss: 1.276, learning rate: 0.008145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13800 (epoch 4.42), 156.2 ms\n",
      "Minibatch loss: 1.134, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.4%\n",
      "Step 13900 (epoch 4.45), 150.1 ms\n",
      "Minibatch loss: 1.174, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Step 14000 (epoch 4.48), 151.7 ms\n",
      "Minibatch loss: 1.046, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.4%\n",
      "Step 14100 (epoch 4.51), 149.3 ms\n",
      "Minibatch loss: 1.130, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.4%\n",
      "Step 14200 (epoch 4.54), 156.4 ms\n",
      "Minibatch loss: 1.182, learning rate: 0.008145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.5%\n",
      "Step 14300 (epoch 4.58), 157.1 ms\n",
      "Minibatch loss: 1.126, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Step 14400 (epoch 4.61), 152.2 ms\n",
      "Minibatch loss: 1.007, learning rate: 0.008145\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.5%\n",
      "Step 14500 (epoch 4.64), 148.6 ms\n",
      "Minibatch loss: 1.060, learning rate: 0.008145\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.7%\n",
      "Step 14600 (epoch 4.67), 151.2 ms\n",
      "Minibatch loss: 1.286, learning rate: 0.008145\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.4%\n",
      "Step 14700 (epoch 4.70), 149.7 ms\n",
      "Minibatch loss: 0.904, learning rate: 0.008145\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.5%\n",
      "Step 14800 (epoch 4.74), 148.9 ms\n",
      "Minibatch loss: 1.280, learning rate: 0.008145\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.7%\n",
      "Step 14900 (epoch 4.77), 149.7 ms\n",
      "Minibatch loss: 1.026, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.5%\n",
      "Step 15000 (epoch 4.80), 152.6 ms\n",
      "Minibatch loss: 1.122, learning rate: 0.008145\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.6%\n",
      "Step 15100 (epoch 4.83), 154.8 ms\n",
      "Minibatch loss: 1.049, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.5%\n",
      "Step 15200 (epoch 4.86), 158.4 ms\n",
      "Minibatch loss: 1.107, learning rate: 0.008145\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Step 15300 (epoch 4.90), 161.3 ms\n",
      "Minibatch loss: 1.214, learning rate: 0.008145\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.6%\n",
      "Step 15400 (epoch 4.93), 155.0 ms\n",
      "Minibatch loss: 1.045, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Step 15500 (epoch 4.96), 168.7 ms\n",
      "Minibatch loss: 1.043, learning rate: 0.008145\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15600 (epoch 4.99), 165.2 ms\n",
      "Minibatch loss: 1.066, learning rate: 0.008145\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.7%\n",
      "Step 15700 (epoch 5.02), 165.6 ms\n",
      "Minibatch loss: 1.137, learning rate: 0.007738\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.6%\n",
      "Step 15800 (epoch 5.06), 157.4 ms\n",
      "Minibatch loss: 1.070, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Step 15900 (epoch 5.09), 164.4 ms\n",
      "Minibatch loss: 0.909, learning rate: 0.007738\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.5%\n",
      "Step 16000 (epoch 5.12), 166.5 ms\n",
      "Minibatch loss: 1.060, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.4%\n",
      "Step 16100 (epoch 5.15), 160.7 ms\n",
      "Minibatch loss: 1.058, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Step 16200 (epoch 5.18), 159.1 ms\n",
      "Minibatch loss: 0.937, learning rate: 0.007738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.3%\n",
      "Step 16300 (epoch 5.22), 156.7 ms\n",
      "Minibatch loss: 1.076, learning rate: 0.007738\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.7%\n",
      "Step 16400 (epoch 5.25), 164.7 ms\n",
      "Minibatch loss: 0.999, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Step 16500 (epoch 5.28), 160.4 ms\n",
      "Minibatch loss: 0.929, learning rate: 0.007738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Step 16600 (epoch 5.31), 157.0 ms\n",
      "Minibatch loss: 1.042, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Step 16700 (epoch 5.34), 157.3 ms\n",
      "Minibatch loss: 1.088, learning rate: 0.007738\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.5%\n",
      "Step 16800 (epoch 5.38), 148.8 ms\n",
      "Minibatch loss: 1.070, learning rate: 0.007738\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.7%\n",
      "Step 16900 (epoch 5.41), 151.1 ms\n",
      "Minibatch loss: 1.028, learning rate: 0.007738\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 91.6%\n",
      "Step 17000 (epoch 5.44), 149.5 ms\n",
      "Minibatch loss: 1.006, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17100 (epoch 5.47), 150.1 ms\n",
      "Minibatch loss: 1.033, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.5%\n",
      "Step 17200 (epoch 5.50), 152.3 ms\n",
      "Minibatch loss: 0.897, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17300 (epoch 5.54), 152.8 ms\n",
      "Minibatch loss: 0.949, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.7%\n",
      "Step 17400 (epoch 5.57), 155.5 ms\n",
      "Minibatch loss: 0.975, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n",
      "Step 17500 (epoch 5.60), 153.0 ms\n",
      "Minibatch loss: 0.890, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17600 (epoch 5.63), 161.9 ms\n",
      "Minibatch loss: 0.992, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Step 17700 (epoch 5.66), 169.7 ms\n",
      "Minibatch loss: 0.924, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.6%\n",
      "Step 17800 (epoch 5.70), 157.0 ms\n",
      "Minibatch loss: 1.002, learning rate: 0.007738\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 91.7%\n",
      "Step 17900 (epoch 5.73), 154.6 ms\n",
      "Minibatch loss: 0.925, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Step 18000 (epoch 5.76), 153.9 ms\n",
      "Minibatch loss: 0.950, learning rate: 0.007738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.8%\n",
      "Step 18100 (epoch 5.79), 156.0 ms\n",
      "Minibatch loss: 0.857, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Step 18200 (epoch 5.82), 154.2 ms\n",
      "Minibatch loss: 0.906, learning rate: 0.007738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18300 (epoch 5.86), 158.5 ms\n",
      "Minibatch loss: 0.890, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18400 (epoch 5.89), 155.1 ms\n",
      "Minibatch loss: 0.879, learning rate: 0.007738\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18500 (epoch 5.92), 154.6 ms\n",
      "Minibatch loss: 0.819, learning rate: 0.007738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Step 18600 (epoch 5.95), 153.5 ms\n",
      "Minibatch loss: 0.836, learning rate: 0.007738\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.1%\n",
      "Step 18700 (epoch 5.98), 156.8 ms\n",
      "Minibatch loss: 0.759, learning rate: 0.007738\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18800 (epoch 6.02), 157.1 ms\n",
      "Minibatch loss: 0.814, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Step 18900 (epoch 6.05), 155.1 ms\n",
      "Minibatch loss: 0.908, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Step 19000 (epoch 6.08), 155.2 ms\n",
      "Minibatch loss: 0.764, learning rate: 0.007351\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.7%\n",
      "Step 19100 (epoch 6.11), 156.6 ms\n",
      "Minibatch loss: 0.984, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.8%\n",
      "Step 19200 (epoch 6.14), 169.1 ms\n",
      "Minibatch loss: 0.784, learning rate: 0.007351\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.9%\n",
      "Step 19300 (epoch 6.18), 163.1 ms\n",
      "Minibatch loss: 0.806, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Step 19400 (epoch 6.21), 166.1 ms\n",
      "Minibatch loss: 0.739, learning rate: 0.007351\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.8%\n",
      "Step 19500 (epoch 6.24), 158.5 ms\n",
      "Minibatch loss: 0.828, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 91.8%\n",
      "Step 19600 (epoch 6.27), 165.8 ms\n",
      "Minibatch loss: 0.785, learning rate: 0.007351\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.0%\n",
      "Step 19700 (epoch 6.30), 150.6 ms\n",
      "Minibatch loss: 0.920, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.9%\n",
      "Step 19800 (epoch 6.34), 153.5 ms\n",
      "Minibatch loss: 0.774, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Step 19900 (epoch 6.37), 154.6 ms\n",
      "Minibatch loss: 0.766, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.7%\n",
      "Step 20000 (epoch 6.40), 150.5 ms\n",
      "Minibatch loss: 0.696, learning rate: 0.007351\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.9%\n",
      "Step 20100 (epoch 6.43), 155.2 ms\n",
      "Minibatch loss: 1.108, learning rate: 0.007351\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20200 (epoch 6.46), 149.2 ms\n",
      "Minibatch loss: 0.908, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20300 (epoch 6.50), 150.5 ms\n",
      "Minibatch loss: 0.854, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.1%\n",
      "Step 20400 (epoch 6.53), 148.7 ms\n",
      "Minibatch loss: 0.920, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20500 (epoch 6.56), 150.0 ms\n",
      "Minibatch loss: 0.778, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Step 20600 (epoch 6.59), 150.3 ms\n",
      "Minibatch loss: 0.716, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20700 (epoch 6.62), 151.9 ms\n",
      "Minibatch loss: 0.645, learning rate: 0.007351\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.0%\n",
      "Step 20800 (epoch 6.66), 153.2 ms\n",
      "Minibatch loss: 0.810, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 91.8%\n",
      "Step 20900 (epoch 6.69), 150.5 ms\n",
      "Minibatch loss: 0.802, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.0%\n",
      "Step 21000 (epoch 6.72), 148.6 ms\n",
      "Minibatch loss: 0.815, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.1%\n",
      "Step 21100 (epoch 6.75), 148.2 ms\n",
      "Minibatch loss: 0.798, learning rate: 0.007351\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Step 21200 (epoch 6.78), 149.1 ms\n",
      "Minibatch loss: 0.682, learning rate: 0.007351\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21300 (epoch 6.82), 148.4 ms\n",
      "Minibatch loss: 0.708, learning rate: 0.007351\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21400 (epoch 6.85), 150.5 ms\n",
      "Minibatch loss: 0.810, learning rate: 0.007351\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Step 21500 (epoch 6.88), 148.3 ms\n",
      "Minibatch loss: 0.761, learning rate: 0.007351\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.9%\n",
      "Step 21600 (epoch 6.91), 148.1 ms\n",
      "Minibatch loss: 0.685, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Step 21700 (epoch 6.94), 148.2 ms\n",
      "Minibatch loss: 0.683, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 21800 (epoch 6.98), 149.3 ms\n",
      "Minibatch loss: 0.784, learning rate: 0.007351\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 21900 (epoch 7.01), 149.8 ms\n",
      "Minibatch loss: 0.820, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.0%\n",
      "Step 22000 (epoch 7.04), 148.7 ms\n",
      "Minibatch loss: 0.629, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.1%\n",
      "Step 22100 (epoch 7.07), 152.6 ms\n",
      "Minibatch loss: 0.783, learning rate: 0.006983\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.0%\n",
      "Step 22200 (epoch 7.10), 150.8 ms\n",
      "Minibatch loss: 0.841, learning rate: 0.006983\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.2%\n",
      "Step 22300 (epoch 7.14), 149.5 ms\n",
      "Minibatch loss: 0.674, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 22400 (epoch 7.17), 150.1 ms\n",
      "Minibatch loss: 0.870, learning rate: 0.006983\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.1%\n",
      "Step 22500 (epoch 7.20), 148.8 ms\n",
      "Minibatch loss: 0.715, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Step 22600 (epoch 7.23), 152.7 ms\n",
      "Minibatch loss: 0.816, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 22700 (epoch 7.26), 151.3 ms\n",
      "Minibatch loss: 0.828, learning rate: 0.006983\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.2%\n",
      "Step 22800 (epoch 7.30), 149.9 ms\n",
      "Minibatch loss: 0.684, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.1%\n",
      "Step 22900 (epoch 7.33), 149.0 ms\n",
      "Minibatch loss: 0.703, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Step 23000 (epoch 7.36), 151.6 ms\n",
      "Minibatch loss: 0.755, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Step 23100 (epoch 7.39), 151.3 ms\n",
      "Minibatch loss: 0.619, learning rate: 0.006983\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.2%\n",
      "Step 23200 (epoch 7.42), 150.8 ms\n",
      "Minibatch loss: 1.120, learning rate: 0.006983\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 92.1%\n",
      "Step 23300 (epoch 7.46), 149.3 ms\n",
      "Minibatch loss: 0.713, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Step 23400 (epoch 7.49), 150.0 ms\n",
      "Minibatch loss: 0.656, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.2%\n",
      "Step 23500 (epoch 7.52), 149.2 ms\n",
      "Minibatch loss: 0.735, learning rate: 0.006983\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.2%\n",
      "Step 23600 (epoch 7.55), 149.8 ms\n",
      "Minibatch loss: 0.584, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.1%\n",
      "Step 23700 (epoch 7.58), 152.0 ms\n",
      "Minibatch loss: 0.546, learning rate: 0.006983\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.3%\n",
      "Step 23800 (epoch 7.62), 159.5 ms\n",
      "Minibatch loss: 0.679, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 23900 (epoch 7.65), 153.5 ms\n",
      "Minibatch loss: 0.633, learning rate: 0.006983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24000 (epoch 7.68), 149.5 ms\n",
      "Minibatch loss: 0.771, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Step 24100 (epoch 7.71), 154.5 ms\n",
      "Minibatch loss: 0.563, learning rate: 0.006983\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Step 24200 (epoch 7.74), 156.5 ms\n",
      "Minibatch loss: 0.653, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.4%\n",
      "Step 24300 (epoch 7.78), 149.8 ms\n",
      "Minibatch loss: 0.704, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24400 (epoch 7.81), 149.5 ms\n",
      "Minibatch loss: 0.578, learning rate: 0.006983\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Step 24500 (epoch 7.84), 149.0 ms\n",
      "Minibatch loss: 0.944, learning rate: 0.006983\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24600 (epoch 7.87), 155.5 ms\n",
      "Minibatch loss: 0.679, learning rate: 0.006983\n",
      "Minibatch accuracy: 90.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 92.1%\n",
      "Step 24700 (epoch 7.90), 151.0 ms\n",
      "Minibatch loss: 0.563, learning rate: 0.006983\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Step 24800 (epoch 7.94), 153.0 ms\n",
      "Minibatch loss: 0.739, learning rate: 0.006983\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.3%\n",
      "Step 24900 (epoch 7.97), 151.0 ms\n",
      "Minibatch loss: 0.606, learning rate: 0.006983\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 25000 (epoch 8.00), 156.4 ms\n",
      "Minibatch loss: 0.686, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25100 (epoch 8.03), 151.7 ms\n",
      "Minibatch loss: 0.732, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.2%\n",
      "Step 25200 (epoch 8.06), 162.3 ms\n",
      "Minibatch loss: 0.555, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 25300 (epoch 8.10), 170.6 ms\n",
      "Minibatch loss: 0.604, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Step 25400 (epoch 8.13), 151.9 ms\n",
      "Minibatch loss: 0.760, learning rate: 0.006634\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 92.4%\n",
      "Step 25500 (epoch 8.16), 151.8 ms\n",
      "Minibatch loss: 0.564, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 25600 (epoch 8.19), 155.5 ms\n",
      "Minibatch loss: 0.623, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 25700 (epoch 8.22), 159.8 ms\n",
      "Minibatch loss: 0.555, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.5%\n",
      "Step 25800 (epoch 8.26), 150.4 ms\n",
      "Minibatch loss: 0.637, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Step 25900 (epoch 8.29), 153.5 ms\n",
      "Minibatch loss: 0.732, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.4%\n",
      "Step 26000 (epoch 8.32), 149.0 ms\n",
      "Minibatch loss: 0.577, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.4%\n",
      "Step 26100 (epoch 8.35), 151.1 ms\n",
      "Minibatch loss: 0.572, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26200 (epoch 8.38), 150.6 ms\n",
      "Minibatch loss: 0.660, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.2%\n",
      "Step 26300 (epoch 8.42), 154.1 ms\n",
      "Minibatch loss: 0.632, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26400 (epoch 8.45), 160.7 ms\n",
      "Minibatch loss: 0.626, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26500 (epoch 8.48), 158.6 ms\n",
      "Minibatch loss: 0.647, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 26600 (epoch 8.51), 152.2 ms\n",
      "Minibatch loss: 0.534, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 26700 (epoch 8.54), 154.7 ms\n",
      "Minibatch loss: 0.521, learning rate: 0.006634\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Step 26800 (epoch 8.58), 151.5 ms\n",
      "Minibatch loss: 0.692, learning rate: 0.006634\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.5%\n",
      "Step 26900 (epoch 8.61), 151.8 ms\n",
      "Minibatch loss: 0.688, learning rate: 0.006634\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 92.6%\n",
      "Step 27000 (epoch 8.64), 151.9 ms\n",
      "Minibatch loss: 0.692, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.7%\n",
      "Step 27100 (epoch 8.67), 150.0 ms\n",
      "Minibatch loss: 0.582, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.4%\n",
      "Step 27200 (epoch 8.70), 149.1 ms\n",
      "Minibatch loss: 0.582, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 27300 (epoch 8.74), 150.7 ms\n",
      "Minibatch loss: 0.478, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.3%\n",
      "Step 27400 (epoch 8.77), 151.4 ms\n",
      "Minibatch loss: 0.575, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Step 27500 (epoch 8.80), 149.9 ms\n",
      "Minibatch loss: 0.656, learning rate: 0.006634\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.5%\n",
      "Step 27600 (epoch 8.83), 150.0 ms\n",
      "Minibatch loss: 0.438, learning rate: 0.006634\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 92.4%\n",
      "Step 27700 (epoch 8.86), 150.6 ms\n",
      "Minibatch loss: 0.589, learning rate: 0.006634\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Step 27800 (epoch 8.90), 150.1 ms\n",
      "Minibatch loss: 0.512, learning rate: 0.006634\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.6%\n",
      "Step 27900 (epoch 8.93), 149.1 ms\n",
      "Minibatch loss: 0.672, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28000 (epoch 8.96), 148.3 ms\n",
      "Minibatch loss: 0.598, learning rate: 0.006634\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.6%\n",
      "Step 28100 (epoch 8.99), 150.0 ms\n",
      "Minibatch loss: 0.598, learning rate: 0.006634\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28200 (epoch 9.02), 148.5 ms\n",
      "Minibatch loss: 0.650, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28300 (epoch 9.06), 150.7 ms\n",
      "Minibatch loss: 0.572, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.3%\n",
      "Step 28400 (epoch 9.09), 148.6 ms\n",
      "Minibatch loss: 0.527, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Step 28500 (epoch 9.12), 149.0 ms\n",
      "Minibatch loss: 0.511, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28600 (epoch 9.15), 151.7 ms\n",
      "Minibatch loss: 0.629, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.5%\n",
      "Step 28700 (epoch 9.18), 158.8 ms\n",
      "Minibatch loss: 0.611, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.4%\n",
      "Step 28800 (epoch 9.22), 148.0 ms\n",
      "Minibatch loss: 0.477, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.6%\n",
      "Step 28900 (epoch 9.25), 148.5 ms\n",
      "Minibatch loss: 0.572, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.6%\n",
      "Step 29000 (epoch 9.28), 148.7 ms\n",
      "Minibatch loss: 0.451, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29100 (epoch 9.31), 147.9 ms\n",
      "Minibatch loss: 0.509, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29200 (epoch 9.34), 150.1 ms\n",
      "Minibatch loss: 0.540, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29300 (epoch 9.38), 148.4 ms\n",
      "Minibatch loss: 0.389, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29400 (epoch 9.41), 148.3 ms\n",
      "Minibatch loss: 0.403, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.4%\n",
      "Step 29500 (epoch 9.44), 151.1 ms\n",
      "Minibatch loss: 0.513, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.6%\n",
      "Step 29600 (epoch 9.47), 149.0 ms\n",
      "Minibatch loss: 0.566, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.4%\n",
      "Step 29700 (epoch 9.50), 149.1 ms\n",
      "Minibatch loss: 0.660, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.6%\n",
      "Step 29800 (epoch 9.54), 148.0 ms\n",
      "Minibatch loss: 0.571, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.5%\n",
      "Step 29900 (epoch 9.57), 148.4 ms\n",
      "Minibatch loss: 0.560, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 30000 (epoch 9.60), 149.0 ms\n",
      "Minibatch loss: 0.535, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 30100 (epoch 9.63), 150.5 ms\n",
      "Minibatch loss: 0.523, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.7%\n",
      "Step 30200 (epoch 9.66), 152.2 ms\n",
      "Minibatch loss: 0.537, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.6%\n",
      "Step 30300 (epoch 9.70), 151.1 ms\n",
      "Minibatch loss: 0.539, learning rate: 0.006302\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 92.5%\n",
      "Step 30400 (epoch 9.73), 150.1 ms\n",
      "Minibatch loss: 0.492, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.6%\n",
      "Step 30500 (epoch 9.76), 151.6 ms\n",
      "Minibatch loss: 0.554, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.7%\n",
      "Step 30600 (epoch 9.79), 157.2 ms\n",
      "Minibatch loss: 0.448, learning rate: 0.006302\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 92.7%\n",
      "Step 30700 (epoch 9.82), 156.6 ms\n",
      "Minibatch loss: 0.698, learning rate: 0.006302\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 92.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30800 (epoch 9.86), 159.8 ms\n",
      "Minibatch loss: 0.485, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.7%\n",
      "Step 30900 (epoch 9.89), 161.2 ms\n",
      "Minibatch loss: 0.580, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.5%\n",
      "Step 31000 (epoch 9.92), 154.4 ms\n",
      "Minibatch loss: 0.557, learning rate: 0.006302\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.5%\n",
      "Step 31100 (epoch 9.95), 153.0 ms\n",
      "Minibatch loss: 0.670, learning rate: 0.006302\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 92.6%\n",
      "Step 31200 (epoch 9.98), 154.4 ms\n",
      "Minibatch loss: 0.427, learning rate: 0.006302\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.6%\n",
      "Test accuracy: 97.2%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "batch_size=64\n",
    "PIXEL_DEPTH = 255\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_FREQUENCY = 100  # Number of steps between evaluations.\n",
    "\n",
    "def lenet(argv=None):  # pylint: disable=unused-argument\n",
    "  train_size = train_labels.shape[0]\n",
    "\n",
    "  # This is where training samples and labels are fed to the graph.\n",
    "  # These placeholder nodes will be fed a batch of training data at each\n",
    "  # training step using the {feed_dict} argument to the Run() call below.\n",
    "  train_data_node = tf.placeholder(\n",
    "      tf.float32,\n",
    "      shape=(batch_size, image_size, image_size, num_channels))\n",
    "  train_labels_node = tf.placeholder(tf.float32,\n",
    "                                     shape=(batch_size, num_labels))\n",
    "  eval_data = tf.placeholder(\n",
    "      tf.float32,\n",
    "      shape=(EVAL_BATCH_SIZE, image_size, image_size, num_channels))\n",
    "\n",
    "  # The variables below hold all the trainable weights. They are passed an\n",
    "  # initial value which will be assigned when when we call:\n",
    "  # {tf.initialize_all_variables().run()}\n",
    "  conv1_weights = tf.Variable(\n",
    "      tf.truncated_normal([5, 5, num_channels, 32],  # 5x5 filter, depth 32.\n",
    "                          stddev=0.1,\n",
    "                          seed=SEED))\n",
    "  conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "  conv2_weights = tf.Variable(\n",
    "      tf.truncated_normal([5, 5, 32, 64],\n",
    "                          stddev=0.1,\n",
    "                          seed=SEED))\n",
    "  conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "  fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "      tf.truncated_normal(\n",
    "          [image_size // 4 * image_size // 4 * 64, 512],\n",
    "          stddev=0.1,\n",
    "          seed=SEED))\n",
    "  fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "  fc2_weights = tf.Variable(\n",
    "      tf.truncated_normal([512, num_labels],\n",
    "                          stddev=0.1,\n",
    "                          seed=SEED))\n",
    "  fc2_biases = tf.Variable(tf.constant(0.1, shape=[num_labels]))\n",
    "\n",
    "  # We will replicate the model structure for the training subgraph, as well\n",
    "  # as the evaluation subgraphs, while sharing the trainable parameters.\n",
    "  def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "    # Max pooling. The kernel size spec {ksize} also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "      hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "  # Training computation: logits + cross-entropy loss.\n",
    "  logits = model(train_data_node, True)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "      logits=logits, labels=train_labels_node))\n",
    "\n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                  tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 5e-4 * regularizers\n",
    "\n",
    "  # Optimizer: set up a variable that's incremented once per batch and\n",
    "  # controls the learning rate decay.\n",
    "  batch = tf.Variable(0)\n",
    "  # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "      0.01,                # Base learning rate.\n",
    "      batch * batch_size,  # Current index into the dataset.\n",
    "      train_size,          # Decay step.\n",
    "      0.95,                # Decay rate.\n",
    "      staircase=True)\n",
    "  # Use simple momentum for the optimization.\n",
    "  optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                         0.9).minimize(loss,\n",
    "                                                       global_step=batch)\n",
    "\n",
    "  # Predictions for the current training minibatch.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  # Predictions for the test and validation, which we'll compute less often.\n",
    "  eval_prediction = tf.nn.softmax(model(eval_data))\n",
    "\n",
    "  # Small utility function to evaluate a dataset by feeding batches of data to\n",
    "  # {eval_data} and pulling the results from {eval_predictions}.\n",
    "  # Saves memory and enables this to run on smaller GPUs.\n",
    "  def eval_in_batches(data, sess):\n",
    "    \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n",
    "    size = data.shape[0]\n",
    "    if size < EVAL_BATCH_SIZE:\n",
    "      raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "    predictions = np.ndarray(shape=(size, num_labels), dtype=np.float32)\n",
    "    for begin in range(0, size, EVAL_BATCH_SIZE):\n",
    "      end = begin + EVAL_BATCH_SIZE\n",
    "      if end <= size:\n",
    "        predictions[begin:end, :] = sess.run(\n",
    "            eval_prediction,\n",
    "            feed_dict={eval_data: data[begin:end, ...]})\n",
    "      else:\n",
    "        batch_predictions = sess.run(\n",
    "            eval_prediction,\n",
    "            feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
    "        predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
    "    return predictions\n",
    "\n",
    "  # Create a local session to run the training.\n",
    "  start_time = time.time()\n",
    "  with tf.Session() as sess:\n",
    "    # Run all the initializers to prepare the trainable parameters.\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized!')\n",
    "    # Loop through training steps.\n",
    "    for step in range(int(NUM_EPOCHS * train_size) // batch_size):\n",
    "      # Compute the offset of the current minibatch in the data.\n",
    "      # Note that we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_size - batch_size)\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), ...]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size)]\n",
    "      # This dictionary maps the batch data (as a numpy array) to the\n",
    "      # node in the graph is should be fed to.\n",
    "      feed_dict = {train_data_node: batch_data,\n",
    "                   train_labels_node: batch_labels}\n",
    "      # Run the graph and fetch some of the nodes.\n",
    "      _, l, lr, predictions = sess.run(\n",
    "          [optimizer, loss, learning_rate, train_prediction],\n",
    "          feed_dict=feed_dict)\n",
    "      if step % EVAL_FREQUENCY == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        print('Step %d (epoch %.2f), %.1f ms' %\n",
    "              (step, float(step) * batch_size / train_size,\n",
    "               1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "        print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "        print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "        print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            eval_in_batches(valid_dataset, sess), valid_labels))\n",
    "        sys.stdout.flush()\n",
    "    # Finally print the result!\n",
    "    test_error = accuracy(eval_in_batches(test_dataset, sess), test_labels)\n",
    "    print('Test accuracy: %.1f%%' % test_error)\n",
    "    \n",
    "lenet()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
