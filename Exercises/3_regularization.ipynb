{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-b3ae065e0ec2>:28: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)\n",
    "      + beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 666.860168\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 28.7%\n",
      "Minibatch loss at step 500: 198.203827\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 1000: 115.310806\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1500: 67.863762\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2000: 41.469334\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 2500: 25.379519\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 3000: 15.310007\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 86.8%\n",
      "Test accuracy: 92.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 0.001}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to try many different values of the regualarzation beta value to see what works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val2 = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val2.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEMCAYAAADUEk3/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XlcVPX+x/HXh00UEFfAfV9zQ3HLVGzftDQttW7Zom3erO69Lbfuz9u9bbd982a2lyWatlhZ2iJquYKa4q6ooOKGAgKCIN/fH3PoTogwAsOZGT7Px4OHzFk/M/Pl7Znv+c45YoxBKaWU9/OzuwCllFJVQwNdKaV8hAa6Ukr5CA10pZTyERroSinlIzTQlVLKR2igK9uISLCIGBFpbnct50pEVorITZVYf5eIDKzimmqJSLaINK3K7Tpt/2URucv6/XIR2VkF26xwzSLyhIi84cJy/xWRCRUq0MtooJfBamjFP0UictLp8Y2V2G6lwkB5P2NMO2PMispso2Q7MsbkG2NCjTEHKl/hGftqBowG3qvK7bpac2n/gRhjphpjJruwm+eAf4qIf2Vq9QYa6GWwGlqoMSYUSAGGO037xO763EVEAuyuobI89Tl4al0uuA340hhzyu5CzpUxZg+QClxhcylup4FeCSLiLyL/EJFkETkqIp+ISD1rXoiIxInIMRHJEJFVIlJfRF4E+gLvWEf6L5ay3QARmScih6x1F4tIJ6f5ISLymoikikimiCwpDgoRibWO3DJFJEVExlvT/3A0JyJ3iciP1u/FXR93i8guIMma/qaI7BORLBFZLSIDStQ41XruWSKyRkSiRORdEXmqxPP5ofij+llcKyJ7ROSIiDwlDnWs7XZw2k5zEcktfo1L7OMuEflZRKaJyHHgEWv6nSKyzXofvrWONIvXuUpEdliv8SvOr5GIPCsi7zgt21lECksr3poXb+3jiIh8KCJhTvMPishfRWQTkOU07QKrDTl/Esyx3osoEWksIt9Z2zwmIl+JSBNr/TPakZTowhKRBiLyqbX+bhF5SETE6fX6yWpHGeLoArq4jPfoCmDJ2WaKSHcRWWZta4OIXOE0L8J6HlnWa/xsKW2vuOZrRGSriJyw2vd9ItIQ+AJo6/Q6NSzlPSq17VvigavKeH6+wRijPy78AHuAi0tMewRYBjQFgoEPgPeteVOAuUBtIADHH1+INW8lcFMZ+woAbgFCre2+Cax0mv8usAiIAvyBwda/7YFs4DprG42BnqXtE7gL+NH6PRgwwLdAPaC2Nf1moD4QCDyG4ygn0Jr3D2CdtU8/INpadwiwGxBruaZALtCglOdZvN+F1rptgOTiOnF8vH/CafmHgc/O8prdBRQCE63XojYwFtgCdLSew5PAYmv5KOu1utqa9xBQ4LTvZ4F3nLbfGSh0erzSadnOwIVAkLXdlcCzTsseBNZYr0Vtp2kXlPI8XgJ+tJ5DJHCN9VzCga+AuNJqKPF6NrcezwE+s9pRe+t9udHp9Sqw3mN/4AFgTxlt8gTQ3enx5cBOp/3uBf5ivZaXWa9tG2v+l8BH1vPoAaRxZtsrrjkd6Gf93hCILrk/pxp+f48oo+1b88cDy+3OEXf/2F6At/xQeqDvBgY5PW6DI7wEuAfHEU23UrZVZqCXsnwUUGQ1/kDrD7FTKcs9Acw6yzZcCfTzy6hBrOfWyXq8F7jsLMslA4Otx38FPj/LNov3G+s07UHgW+v3oc5/xMBGYMRZtnUXsL3EtMXFAWY9Ln7tIoFJWOFuzfMDDlOBQC+llrHACqfHB4HxJZY5I9BxhOtOSvnPz5o/AEgr4z39PRyBWsBpoK3T/CnA906vV5LTvAbWuvVK2a+/Na+10zTnQL/Eag/iNP8LHAc8wVbbbeU074VS2l5xoB8CbgXCStRQXqCfte1b84cDm139m/PWH+1yqSDro2sLYIH1MTMDxxGrH44ji3dxBPpcq9viaXHxpIzVnfFCcXcGsBVHUDYEmuA4AkkuZdUWwK5KPK3UEnU8anVXZALHcfzxNbKee7PS9mUcfz0fAcXdOzcBH5/DfvfiOJIFWAr4i8hAEemF47l/52r9QCtgutP7cwTHUXxzax+/L2+MKQL2l1NnqUSkqYh8JiL7rffrHaBRObWV3EY/4EXgWmPMMWtamIi8Z3UfZOH4VFZyu2cThaMtpjhN24vjfSt20On3XOvf0JIbMsacxnGEHlZynqUpkGK99yX3FYWj7e5zmlfWa3EtjqPsFKsLLaaMZZ2V1/bDgAwXt+W1NNAryGq8+4ELjTH1nH6CjTFHjePs/f8ZYzrj6IYYg+PIDRxHJGW5FbgUGIbjo3Zna7rg+LhaCLQtZb1UoN1ZtpkD1HF6HFXa0yr+RUQuAf4MjMTRHdIAOInjKKz4uZ9tXx8Bo0WkD44/tG/PslyxFk6/twQOwBn/OfwJR3dDQRnbKfm6pgITSrw/tY0xiThex9+HS4qIH38MO1der2LPW8t3M8bUBe7A8V6VVdvvrH7xz4E7jDFJTrMesWrsa2330hLbLasdHcRxZNzSaVpLKvifFrABR9dVaQ6U2I/zvg7iqNP5tW3BWRhjVhhjrsbxKWoRMKt4Vjn1ldX2AboAv5WzDa+ngV4504FnRaQF/H7yZ7j1+8Ui0tUKiiwcIXzaWu8QpQdysTAgD0d/YgiOvl8ArED7CHhVRCKtk2oXWEf/HwFXi8hIa3pjEelhrboeR8gGi0hnYEI5zy0MR/fEERx9w//CcYRe7B3gaRFpKw7RYp2sNMYkA5uB94HZpvyREQ+LSLiItAYmA7Od5n0EXA+Ms34/F9OBx8U6oSyOk9LXWfPmA/1F5EpxnFB+EMf5gmLrgWEi0kxE6uPovz+bMBz9t1ki0tLalktEJAhH98RbxpivStluLpAhIo2Ax0vMP2s7MsbkW9t9Whwn0dvh6HKZ6WptJSzA0QVWmmWAn4jcb326vATHfz6fGWPygK+BJ6y21w1Hf/YZrDrHikhdHG3vBH/8m4kQkTM+QVjKavtYtZf16c432N3n4y0/lN6H7o/jD30Hjsa3E5hqzbvFmp6D4yjlRcDPmjfUWvY48Fwp+wrHcVSbjaOffgJ/7GcMAabhODLKwNFXHGDNuxDHCbgsHB97x1nTI4GfrTqX4vhPotR+TGtaII6ukiwcR1r349Tva83/l/W6nABWAZFO699hbXNgGa9p8X4nW9s5iqNf1K/Ecr8A28p5f34/J1Bi+u1A8eiSvcB0p3kjrPchA3gFWAuMseb5AW8DmcA24E7OflK0F47/ALKBRKtNOPf9l9ZffhC4AMenL2Ot6/wTgeMo9xfr8VYc52Wca/hDOyr5PuLooouzXte9wKP872T1H16v0tpAiXqb4ui+CbIe/6FPG+hp1ZqJ41zHVU7zonCc+D5hvW4v8r/zJM79/iE4jsqPW+/XKqC/tZzg+M8o3Xq/GnDmeY6ztf1W1uMAu3PE3T/Fb65SVUpELgX+a4xpXwXb+hTHCa0ny1244vsIwBGyw00lv/Djq0TkJRwnnqdXcjuvAsHGmDurprJy9zcNSDTGVOmXojyRBrqqclY3wufAUmPMc5XcVnscR85djDEV7f8927avAJYD+TiGZd4CtDde+OUZT2Z1sxgc3XADcXz6HGeM+d7WwnyQ9qGrKmWNRjmOo/93WiW39RyOkUP/quowtxSPmT8MXASM1DB3i3Ac/eg5OLpNntQwdw89QldKKR+hR+hKKeUjNNCVUspHVOuV3xo1amRat25doXVzcnIICQmp2oKUcpG2P2WnxMTEo8aYxuUtV62B3rp1axISEiq0bnx8PLGxsVVbkFIu0van7CQie11ZTrtclFLKR2igK6WUj9BAV0opH6GBrpRSPkIDXSmlfIQGulJK+QgNdKXcLL/wNGv2HKPgdJHdpSgfV63j0JWqafIKTjPxowSW7ThKo9BajO7TnBv6tqBNI/2Skqp6GuhKuUnuqULu+DCBFcnp/PnC9mw9eIK3lyUzfcku+rdpwLh+Lbm8WxTBgS7dalapcmmgK+UGOfmF3PrBGhL2HOPFMT0Z1dtx+9LDWXl8lriP2WtSuX/2eup+FcDI6Gbc0LclXZvWtblq5e000JWqYifyCrj1/TWsS83g5Rt6cU2v/90fOaJuMPcOa8/dQ9uxMjmduDWpzFqdyocr9tKjeThj+7ZkeM8mhAUH2vgMlLfSQFeqCmWeLGDC+6vZuC+T18ZGc1WPJqUu5+cnnN++Eee3b8TxnFN8uX4/catT+fsXG/n3N5u5ukcTxvZrSe+W9RCRan4WyltpoCtVRTJyT3Hze6vZkpbFtBt7c9l5US6tVz8kiFsHtWHC+a1Zn5rB7DWpzP/tAJ8l7qNDRCg39G3BqN7NaRAS5OZnoLydDltUqgoczznF+LdXsTXtBNNv6uNymDsTEaJb1ufZ63qw+rGLeXZUd0JqBfDkt1s4/9mf+Gq9O+7Cp3yJS4EuIlNEJElENonI/da0f4vIBhFZLyKLRKSpe0tVyjMdzc5n3Nsr2Xkkmxk39+GiLpGV3mZorQDG9mvJl/cO4vv7B9OjWT2mxK3nue+3UlSkt41UpSs30K07dk8E+gE9gatFpAPwvDGmhzGmF/AN8H9urVQpD3T4RB7jZqxkT3oO793Sl9hOEVW+j85RdZl5R3/G9WvBf+N3MenjBLLzC6t8P8r7uXKE3gVYaYzJNcYUAktw3B09y2mZEEAPG1SNcigrj7EzVrI/4yTvT+jHBR0auW1fQQF+PD2yO0+MOI/F244w6r+/kpKe67b9Ke8kxpSdwyLSBfgKGAicBH4CEowxfxaRp4CbgUxgmDHmSCnrTwImAURGRvaJi4urUKHZ2dmEhoZWaF2lKqtk+0s/WcRza/LIzDc8GBNMx/rV9+WgzemnmbY+D4DJvYLp0lC/mOTrhg0blmiMiSlvuXIDHUBEbgfuBbKBzcBJY8wDTvMfBYKNMVPL2k5MTIzRW9Apb+Tc/vYdz2Xc2yvJyCngw9v70btl/WqvZ8/RHO74KIE9R3OYOuI8/jSgVbXXoKqPiLgU6C6dFDXGvGuM6W2MGQIcA3aUWORT4LpzL1Mp75KSnssNb60kM7eAmXf0tyXMAVo3CuHze85ncIdG/OPLJB77YqNe/Eu5PMolwvq3JTAKmGWdGC02Atha9eUp5Tl2H83hhhkryDlVyKcTB9CzRT1b66kbHMg7t/TlzqFt+WRVCn96dxXHck7ZWpOyl6vj0OeJyGbga+BeY8xx4FlrKOMG4FJgiruKVMpuadlF3PDWCvILi5g1cQDdmoXbXRIA/n7Co1d04eUberI2JYNrpv3CtoMn7C5L2cSlb4oaYwaXMk27WFSNsPtoDs+sziMoKIi4SQPoGBlmd0lnGBndnNYNQ7jz40RG/fdXXhkbzSVdKz8eXnkX/aaoUmXIKzjN3TMTMcZ4bJgXi25Zn/mTL6BdRCiTPk5g2uKduDLoQfkODXSlyvDkt5vZevAEE3vUon2E5w+bjQoPZs6dAxneoynPL9zGlLj15BWctrssVU000JU6i283pDFzZQp3DmlLj8becx274EB/Xh3bi4cu78TXGw4wZvoKDmbm2V2WqgYa6EqVIvVYLo/M20B0y3r89bJOdpdzzkSEe2Lb8/afYkg+ks3wN34hce9xu8tSbqaBrlQJpwqLmDxrHSLw2thoAv2998/k4q6RfH7PIGoH+jN2xgriVqfYXZJyI+9tqUq5yfMLt/JbagbPje5BiwZ17C6n0jpFhTF/8iAGtG3II59v5LEvNnKqUL+E5Is00JVy8vPWQ7y9bDc3D2zF5d1Kv9uQN6pXJ4gPbu3HXUPb8cmqFMa/vZLDJ7Rf3ddooCtlScs8yV/m/EbXJnX5+5Vd7C6nyvn7CY9c0ZnXx0WTdCCTEa//yvrUDLvLUlVIA10poPB0EVNmrSe/sIg3xkcTHOi7VzAc3rMpn989iAB/4frpK5iTkGp3SaqKaKArBbz20w5W7znGUyO70bax5483r6yuTevy9eQL6NumPg/N3cDUr5L04l4+QANd1XjLdx7l9cU7GdOnOSOjm9tdTrWpHxLEh7f2Y+LgNny4Yi83vrOKo9n5dpelKkEDXdVoR07kM2X2eto1DuWJa86zu5xqF+Dvx2NXdeWVG3rxW2oGI17/hY37Mu0uS1WQBrqqsYqKDA/OWU/WyQLeGB9NnSDv+TZoVbs2uhnz7j4fEeG66cuZl7jP7pJUBWigqxrrraXJLNtxlKnDz6NzVF27y7Fdt2bhzJ88iN4t6/GXz37jX19vplD71b2KBrqqkRL3HuOFRdu4ukcTxvVrYXc5HqNhaC0+vr0/tw5qzXu/7ubm91brTTO8iAa6qnEyck9x36z1NKtXm2dGdUdE7C7JowT6+zF1+Hm8MKYnCXuPM/z1X0jar/3q3kADXdUoxhgemruBwyfyeGN8NGHBgXaX5LFG92nOZ3cOpMgYrntzOS8u2kZ2fqHdZakyaKCrGuXD5XtYtPkQj1zRhR7N7b0nqDfo2aIe8ydfwKXnRfH6zzsZ+txiPl6xR8eseygNdFVjJO3P5OkFW7m4SwS3DWptdzleo3FYLV4fF81X9w6ifUQo//hqE5e9vJTvkw7qHZE8jAa6qhFO5BUw+dO1NAwN4vnRPbXfvAJ6tqhH3KQBvHtLDH5+wl0zExk9fQWJe4/ZXZqyaKArn2eM4bEvkkg9fpLXxkVTPyTI7pK8lohwUZdIvp8ymGdGdSflWC7XvbmCu2cmsvtojt3l1Xg195sUqsZ495fdzP/tAH+7rBN9WzewuxyfEODvx7h+LbmmV1PeXrqbt5bu4ofNh7ixf0vuu6gDDUNr2V1ijeTSEbqITBGRJBHZJCL3W9OeF5GtIrJBRL4QET3DpDzOh8v38OS3W7iiWxR3D21ndzk+p05QAFMu7sCSvw1jbL8WzFyVwtDn45m2eCcnT+nNqatbuYEuIt2AiUA/oCdwtYh0AH4AuhljegDbgUfdWahS52rmyr1Mnb+JS7tG8tq4aPz8tN/cXRqH1eLJa7uz8P4hnN+uIc8v3EbsC4uZsyaV00V64rS6uHKE3gVYaYzJNcYUAkuAkcaYRdZjgJVAzblMnfJ4catTePzLJC7qHMEb43t79X1BvUn7iFBm3BzDnDsH0iS8Ng/N28CVry5j8bbDdpdWI7jSypOAISLSUETqAFcCJb8rfRvwXVUXp1RFfJaQyqNfbGRox8b896beBAVomFe3fm0a8MU95zNtfG/yCk9z6/trmBK3jszcArtL82niyjhSEbkduBfIBjYDJ40xD1jzHgNigFGmlI2JyCRgEkBkZGSfuLi4ChWanZ1NaKjv33hAVc7yA4W8vSGfrg39mNI7mCD/qulm0fZXcYVFhm+TC5i/q4CwIOH2bkF0b6zjMc7FsGHDEo0xMeUt51Kg/2EFkaeBfcaY/4rILcBdwEXGmNzy1o2JiTEJCQnntL9i8fHxxMbGVmhdVTPM/+0A98eto3+bhrw3oS+1g6ruNnLa/iovaX8mD8xez47D2dzYvyV/v7ILIbU02F0hIi4FuqujXCKsf1sCo4BZInI58DAwwpUwV8qdvt2QxgOz1xPTugHvToip0jBXVaNbs3C+/vMFTBzchk9Xp3Dla8tI2KNfSqpKrnYuzhORzcDXwL3GmOPAG0AY8IOIrBeR6e4qUqmyLNx0kClx6+jVoh7vTehbo29U4emCA/157KquxE0cQJExjHlrBc98t4X8Qh3iWBVcavnGmMGlTGtf9eUodW5+3HyIyZ+upVuzcD64tS+h+hHeK/Rv25DvpgzhqW8389aSZJZsO8KL1/fkvKbhdpfm1fT0v/Jai7cd5p5P1tKlSV0+ur2fXgrXy4TWCuCZUT14f0Jf0nNOce20X3nj5x16l6RK0EBXXmnp9iPc+XEiHSJD+fi2/tTVMPdawzpHsOj+IVx6XhQvLNrO6OkrSD6SbXdZXkkDXXmdX3ceZeJHCbRtFMLM2/sTXkfD3NvVDwli2vjevDYumt1Hc7jytWV8uHwPRfot03Oiga68ysrkdG7/cA2tGtbhkzv665UTfcyInk1Z9MAQ+rdpyNT5m/jTe6s4kHHS7rK8hga68hpr9hzjtg/W0KxebT65Y4Be0c9HRdYN5oNb+/LUyG6sS8ngsleWMjdxn95MwwUa6MorJO49zoT3VhNVN5hZEwfQOEzD3JeJCDf2b8V3UwbTOSqMv372G5e+vJTZa1LIK9Ahjmejga483raDJ5jw3moahdXi04kDiKgbbHdJqpq0ahhC3KSBvHxDTwL8/Xh43kYu+M9iXv9pB8dzTtldnsfRQbvK4721ZBcAsyYOICpcw7ym8fcTRkY359pezVi+K523lyXz4g/bmRa/kzF9WnD7BW1o3SjE7jI9gga68mgn8gpYkJTGyOjmNK1X2+5ylI1EhEHtGzGofSO2HzrBO8uSmb0mlZmr9nJp10gmDm5Ln1b1a/T9YjXQlUdbsDGNvIIixsTo5fbV/3SMDOO50T3562Wd+Gj5Xmau2svCTYfo1aIek4a05bLzovCvgTc00T505dHmJu6jbeMQolvoHQ7VmSLCgvnrZZ1Y/siF/Oua8ziee4p7PllL7AuL+eDX3eTkF5a/ER+iga481p6jOazZc5zRfZrX6I/Rqnx1ggK4eWBrfv5LLNNv6kNEWDD//HozA5/5if98v5VDWXl2l1gttMtFeax5a/fhJzAqWrtblGv8/YTLu0VxebcoEvce551lyby1ZBfvLtvN3bHtuHdYe5++g5UGuvJIRUWGeYn7uKBDYx3ZoiqkT6v69GnVh5T0XF78YRuv/rSD75LS+M91PYhuWd/u8tzCd/+rUl5tRXI6BzLzGN1Hj85V5bRsWIdXx0bz3oQYTuQVMurN5fzr683knvK9/nUNdOWR5ibuIyw4gEu7RtpdivIRF3aOZNEDQ7ipfyve+3U3l768lF92HLW7rCqlga48zom8Ar5LSmN4z6YEB+qt5FTVCQsO5N/XdmPOnQMJ8vfjpndX8bfPfiMzt8Du0qqEBrryOMVjz7W7RblLvzYNWDBlMPfEtuPzdfu56KUlLNiY5vUXANNAVx5nbuI+2unYc+VmwYH+PHR5Z+ZPHkRUeC3u+WQtd36c6NVDHDXQlUfZ/fvY8xY69lxVi/OahvPlPYN45IrOLNl+hItfWkLc6hSvPFrXQFceZV6iY+z5yOhmdpeiapAAfz/uGtqO7+8fQtcmdXnk843c+M4q9qbn2F3aOdFAVx7jdJFh3tp9DNax58ombRqFMGviAJ4a2Y2N+zK57JWlvL002WtuXK2BrjzGil3ppOnYc2UzPz/HzTUWPTiEC9o34qkFW7juzeUczc63u7RyuRToIjJFRJJEZJOI3G9NG2M9LhKRGPeWqWqCuYmphAUHcImOPVceoEl4bd6+OYbXx0Wz5eAJHvtio8f3q5cb6CLSDZgI9AN6AleLSAcgCRgFLHVrhapGyMor4PtNBxmhY8+VBxERhvdsyoOXdGThpkN8syHN7pLK5MoRehdgpTEm1xhTCCwBRhpjthhjtrm3PFVTLNigY8+V57rjgjb0bB7O/32V5NFdL65cnCsJeEpEGgIngSuBBFd3ICKTgEkAkZGRxMfHV6BMyM7OrvC6yvO9u/IkTUKEjF3riU/2vOGK2v7U9a2KmLq/gLvfWcy9vTzzpH25gW6M2SIi/wF+ALKB3wCXr2pjjJkBzACIiYkxsbGxFSo0Pj6eiq6rPNvuozns+D6eR67ozLCh7ewup1Ta/hRARuhOnl+4jZwGnbiqRxO7yzmDSydFjTHvGmN6G2OGAMeAHe4tS9UkOvZceYs7h7SlezNH10u6B3a9uDrKJcL6tyWOE6Gz3FmUqjmKx54P6diYyLqe+TFWqWIB/n68MKYnWXkFTJ2/ye5yzuDqOPR5IrIZ+Bq41xhzXERGisg+YCDwrYgsdFuVymct33VUx54rr9IpKoz7LuzANxvS+D7Js0a9uHTHImPM4FKmfQF8UeUVqRplbuI+6gYHcHEXHXuuvMddse34ftNBHv8yif5tGlI/JMjukgD9pqiyUVZeAd8nHWRELx17rrxLoL8fz4/uSUZuAf/82nO6XjTQlW2+3ZBGfmERo/u0sLsUpc5Z16Z1mXxhe75af4BFmw7aXQ6gga5sNDdxH+0jQunZPNzuUpSqkHti29OlSV0e+zKJjNxTdpejga7skXwkm8S9xxndp7le91x5raAAP14Y04PjOaf419eb7S5HA13ZY95ax9jzUTr2XHm585qG/34rux83H7K1Fg10Ve1OFxk+X7ufoR0bE6Fjz5UPmHxhBzpHhfH3LzbaesNpDXRV7f439lxPhirfEBTgGPWSnnOKf39rX9eLBrqqdp8l7CO8diAXdYmwuxSlqkz35uHcPbQdcxP3sXjrYVtq0EBX1SrzZAEL9brnykf9+aL2dIwM5dHPN5J5svq7XjTQVbX639hz/aq/8j21Avx5fnRPDp/I4ykbul400FW1mpuYSoeIUHro2HPlo3q2qMedQ9sxJ2EfS7YfqdZ9a6CrarPrSDZrUzJ07LnyeVMu6kD7iFAembeBrLzq63rRQFfVZl7iPvz9RK97rnxecKA/z4/uwaGsPJ5ZsKXa9quBrqqFjj1XNU10y/pMHNyWWatT+WXH0WrZpwa6qha/7jzKwSy97rmqWR64pCNtG4fw8LwNZOe7fOfOCtNAV9VibqKOPVc1T3HXy5ET+azYle72/bl0gwulKqN47PkNfVtQK0DHnquapU+rBix7eFi13GJRj9CV2326KkXHnqsarbrul6uBrtxqw74MXvphGxd3iaR7Mx17rpQ7aaArt8nKK2Dyp+toHFqLF8b00LHnSrmZ9qErtzDG8PfPN7I/4ySzJw2gXh3PuImuUr5Mj9CVW8StSeWbDWk8eElHYlo3sLscpWoElwJdRKaISJKIbBKR+61pDUTkBxHZYf1b372lKm+x9WAW/5y/icEdGnH30HZ2l6NUjVFuoItIN2Ai0A/oCVwtIh2AR4CfjDEdgJ+sx6qGyz1VyORP1xEWHMhL1/fCz0/7zZWqLq4coXcBVhpjco0xhcASYCSGCKZ0AAART0lEQVRwDfChtcyHwLXuKVF5k3/O38SuI9m8ckMvGofVsrscpWoUV06KJgFPiUhD4CRwJZAARBpj0gCMMWkiUupXAEVkEjAJIDIykvj4+AoVmp2dXeF1VfVYfqCQORvyGd42kML9ScTvt7uiqqPtT3kDMcaUv5DI7cC9QDawGUew32qMqee0zHFjTJn96DExMSYhIaFChcbHxxMbG1uhdZX7JR/JZvjrv9C1aV1mTRxAgL9vnW/X9qfsJCKJxpiY8pZz6a/OGPOuMaa3MWYIcAzYARwSkSbWzpoA9txET9kur+A0kz9dR2CAH6+Ojfa5MFfKW7g6yiXC+rclMAqYBcwHbrEWuQX4yh0FKs/3zIItbE7L4oXRPWlar7bd5ShVY7n6xaJ5Vh96AXCvMea4iDwLzLG6Y1KAMe4qUnmu75PS+HDFXm6/oA0Xd420uxylajSXAt0YM7iUaenARVVekfIaqcdyeWjuBno0D+fhyzvbXY5SNZ52dqoKKThdxH1x6zAGXh8XTVCANiWl7KbXclEV8sKibaxLyeCN8dG0ahhidzlKKfQIXVVA/LbDvLUkmXH9WnJ1j6Z2l6OUsmigq3NyKCuPB+f8RqfIMKYO72p3OUopJxroymWniwz3x63n5KnTTLsxmuBAvZ2cUp5E+9CVy974eScrktN5fnQP2keE2V2OUqoEPUJXLlmZnM6rP21nZHQzvTeoUh5KA12VKz07nylx62jVMIR/X9tNbyWnlIfSQFfleua7rRzPKeCN8dGE1tJeOqU8lQa6KtOWtCzmrd3HLee34rym4XaXo5Qqgwa6KtMz322lbnAgk4d1sLsUpVQ5NNDVWS3bcYSl248weVh7wusE2l2OUqocGuiqVEVFhmcWbKV5/drcfH4ru8tRSrlAA12V6sv1+9mclsXfLutErQD9ApFS3kADXZ0hr+A0LyzcRvdm4QzXa7Uo5TU00NUZPli+hwOZeTx6ZWf8/HTMuVLeQgNd/cHxnFNMW7yTCztHcH67RnaXo5Q6Bxro6g9e+3kHOfmFPHKF3oFIKW+jga5+tzc9h5kr93J9TAs6RurFt5TyNhro6nfPLdxGgJ8fD1zS0e5SlFIVoIGuAFiXcpxvN6QxcXAbIusG212OUqoCNNAVxji+RNQoNIhJQ9vZXY5SqoJcCnQReUBENolIkojMEpFgEblQRNZa0z4UEb0Mn5f6ccthVu85xpSLO+rVFJXyYuUGuog0A+4DYowx3QB/YDzwITDWmrYXuMWdhSr3KDxdxLPfbaFt4xDG9m1hdzlKqUpwtcslAKhtHYXXAXKAfGPMdmv+D8B1bqhPudnshFR2Hcnh4cs7E+ivPXBKebNy/4KNMfuBF4AUIA3IBOYAgSISYy02GtDDOy+TnV/Iyz/soG/r+lzaNdLucpRSlVRuh6mI1AeuAdoAGcBnwI3AWOBlEakFLAIKz7L+JGASQGRkJPHx8RUqNDs7u8LrqtJ9seMUR7MLuKubsGTJErvL8Wja/pQ3cOUM2MXAbmPMEQAR+Rw43xgzExhsTbsUKHXwsjFmBjADICYmxsTGxlao0Pj4eCq6rjrT4aw87v4pnqu6N+GOa3vbXY7H0/anvIErnaYpwAARqSOOuwNfBGwRkQgA6wj9YWC6+8pUVe3lH3dQWFTEQ5d3srsUpVQVcaUPfRUwF1gLbLTWmQH8TUS2ABuAr40xP7uzUFV1dhw6wew1KdzYvxWtGobYXY5Sqoq4NOjYGDMVmFpi8t+sH+Vl/vP9VkKCArjvIr1PqFK+RMep1TArk9P5ccth7h7WjgYhQXaXo5SqQhroNUhRkeHpBVtoEh7MbYPa2F2OUqqKaaDXIN9sTGPDvkz+cmknggP1PqFK+RoN9Boiv/A0zy/cSpcmdRkZ3czucpRSbqCBXkN8vGIvqcdO8ugVnfHX+4Qq5ZM00GuA9Ox8Xv95J4M7NGJIx8Z2l6OUchO9VqoPOV1k2JOew7aDJ9h68ATbDmax7eAJ9h7LBeDRK7rYXKFSyp000L2QMYYj2flsO3jCKbxPsP3QCfILiwAQgdYNQ+jSpC7XRjfjgvaN6Nq0rs2VK6XcSQPdC2TknmLhpoNsSXME97ZDJziWc+r3+Y1Ca9E5KoybBrSiU1QYnaPC6BARRu0gHcmiVE2ige7h1uw5xn2z1pGWmUftQH86RoZycZcIOkXVpXNUGJ2iwmgUWsvuMpVSHkAD3UMVFRneXLKLl37YTvP6tZl390CiW9THT0eoKKXOQgPdAx3NzueB2etZtuMoV/dowjOjuhMWHGh3WUopD6eB7mGW7zrKlLj1ZJ4s4OmR3RnXrwWOqxYrpVTZNNA9xOkiw+s/7+C1n3bQulEIH93Wjy5NdFSKUsp1Guge4HBWHlPi1rMiOZ1R0c3497XdCKmlb41S6txoaths6fYjPDB7PTmnCnludA/G9GmuXSxKqQrRQLdJ4ekiXv5xO/+N30WHiFDixg+gQ2SY3WUppbyYBroN0jJPct+sdazZc5wbYlrwzxHn6ZeAlFKVpoFezX7eeoi/zPmN/MIiXrmhF9fqpWyVUlVEA72aFJwu4vmF25ixNJkuTeoybXw0bRuH2l2WUsqHaKBXg5T0XO6LW8f61AxuGtCSx6/qqncMUkpVOQ10N8orOM2MpclMW7yTIH8/3hgfzdU9mtpdllLKR2mgu8nS7Uf4v6+S2JOey1Xdm/D41V1oEl7b7rKUUj7MpUAXkQeAOwADbARuBQYBz+O461E2MMEYs9NNdXqNtMyT/PubzSzYeJA21jc+9S5BSqnqUG6gi0gz4D6gqzHmpIjMAcYCfweuMcZsEZF7gMeBCe4s1pMVnC7i/V9388qPOzhdZPjLJR2ZNLQttQK0r1wpVT1c7XIJAGqLSAFQBziA42i9+GIj4da0GmlVcjr/+CqJ7YeyuahzBP8ccR4tGtSxuyylVA0jxpjyFxKZAjwFnAQWGWNuFJHBwJfWtCxggDEmq5R1JwGTACIjI/vExcVVqNDs7GxCQz1rmF9mvmHOtlP8eqCQhsHCTV2DiI7Q0xK+yBPbn6o5hg0blmiMiSlvuXIDXUTqA/OAG4AM4DNgLjAK+I8xZpWI/A3oZIy5o6xtxcTEmISEBBefwh/Fx8cTGxtboXWr2ukiw6er9vLcwm3kFZxm0pC2TB7WQb/t6cM8qf2pmkdEXAp0Vw4nLwZ2G2OOWBv+HMcJ0Z7GmFXWMrOB7ytarDdZn5rBP75MYuP+TAa1b8gTI7rRPkKP3JRS9nMl0FOAASJSB0f3ykVAAjBGRDoaY7YDlwBb3Fem/TJyT/Hcwm3MWp1C49BavDYumuE9muiVEZVSHqPcQLe6VOYCa4FCYB0wA9gHzBORIuA4cJs7C7XT8p1HmTxrHZknC7htUBvuv7iD3hJOKeVxXDqDZ4yZCkwtMfkL68enZecX8uCc36hXJ5CZt/ena1O9i5BSyjPpkIxyvPzDdg6dyOPzm87XMFdKeTQ/uwvwZJsOZPL+r7sZ168l0S3r212OUkqVSQP9LIqKDI99kUT9OkE8fFlnu8tRSqlyaaCfxaw1KaxPzeCxq7oQXkdPgCqlPJ8GeimOZufzn++2MqBtA0bqHYWUUl5CA70UTy/YwsmC0zx5bXcdZ66U8hoa6CWs2JXO52v3c+eQdvoNUKWUV9FAd3KqsIjHv9xIiwa1mXxhe7vLUUqpc6Lj0J28vSyZXUdyeH9CX73np1LK63jFEfruozmsSit06z5S0nN57acdXNEtimGdI9y6L6WUcgevCPTXf97Bm7/l8/6vu92yfWMMU+cnEeAn/N/wrm7Zh1JKuZtXBPrTI7vTO8KfJ77ezEuLtuHKTTnOxcJNB1m87QgPXNJRb+SslPJaXtGHHhzoz729arHwWANe+3knx3JP8cSIbvj7VX5IYXZ+IU98vZkuTeoy4fzWlS9WKaVs4hWBDuDvJ/znuh7UDwnirSXJZOQW8NL1vQgKqNyHjFd+2M7BrDym3dibAH+v+MCilFKl8ppABxARHr2iCw3qBPHMd1vJPFnAW3/qQ52gij2NzQeyeH/5Hsb1a0lvvfiWUsrLeeUh6Z1D2/HcdT34dedRxr+9iuM5p855G0VFhse/3Ei92oF68S2llE/wykAHuL5vC968qQ+b07K4/q0VHMzMO6f1ZyeksjZFL76llPIdXhvoAJedF8UHt/YlLTOP695cTvKRbJfWO5qdz7N68S2llI/x6kAHOL9dI+ImDSCv4DRjpq8gaX9mues8s2AruacKefLabnrxLaWUz/D6QAfo1iycz+4aSHCgP2NnrGTFrvSzLrsyOZ15a/cxaUhb2keEVWOVSinlXj4R6ABtG4cy9+6BNAkP5pb3V7Nw08EzlnFcfCvJcfGtYR1sqFIppdzHZwIdoEl4bebcOZCuTepy98xE5iSk/mH+O78ks/NwNv8a0Y3aQXrxLaWUb3Ep0EXkARHZJCJJIjJLRIJFZJmIrLd+DojIl+4u1hX1Q4L45I7+DGrfiIfmbuCtJbsASD2mF99SSvm2cr+RIyLNgPuArsaYkyIyBxhrjBnstMw84Cv3lXluQmoF8O4tfXlwznqe+W4rx3JOsfNwNv6iF99SSvkuV79iGQDUFpECoA5woHiGiIQBFwK3Vn15FRcU4MerY6OpXyeIt5YmA/D4VV304ltKKZ9VbqAbY/aLyAtACnASWGSMWeS0yEjgJ2NMVmnri8gkYBJAZGQk8fHxFSo0Ozu7QuteGG442SGQPVlFtCnYS3x8SoX2r2q2irY/paqTlHcpWhGpD8wDbgAygM+AucaYmdb874B3jDHzyttZTEyMSUhIqFCh8fHxxMbGVmhdpSpL25+yk4gkGmNiylvOlZOiFwO7jTFHjDEFwOfA+dZOGgL9gG8rU6xSSqnKcyXQU4ABIlJHHF+rvAjYYs0bA3xjjDm3C6kopZSqcuUGujFmFTAXWAtstNaZYc0eC8xyW3VKKaVc5tIoF2PMVGBqKdNjq7ogpZRSFeNT3xRVSqmaTANdKaV8hAa6Ukr5CA10pZTyEeV+sahKdyaSCewoY5Fw4Gx3qGgEHK3yoqpPWc/NW/ZZme1VZN1zWceVZctbxpfbH1R/G9T2d27LlDW/lTGmcblVGGOq7QeYUdH5QEJ11lrdz90b9lmZ7VVk3XNZx5Vla3L7c0d7qO791eT25+pPdXe5fF3J+d7MjudW1fuszPYqsu65rOPKsjW5/UH1Pz9tf+e2TKVfr2rtcqkMEUkwLlzLQCl30PanvIE3nRSdUf4iSrmNtj/l8bzmCF0ppVTZvOkIXSmlVBk00JVSykdooCullI/wmUAXkRARSRSRq+2uRdUsItJFRKaLyFwRudvuelTNZXugi8h7InJYRJJKTL9cRLaJyE4RecSFTT0MzHFPlcpXVUX7M8ZsMcbcBVwP6NBGZRvbR7mIyBAgG/jIGNPNmuYPbAcuAfYBa4BxgD/wTIlN3Ab0wPHV7GDgqDHmm+qpXnm7qmh/xpjDIjICeAR4wxjzaXXVr5Qzl25w4U7GmKUi0rrE5H7ATmNMMoCIxAHXGGOeAc7oUhGRYUAI0BU4KSILjDFFbi1c+YSqaH/WduYD80XkW0ADXdnC9kA/i2ZAqtPjfUD/sy1sjHkMQEQm4DhC1zBXlXFO7U9EYoFRQC1ggVsrU6oMnhroUsq0cvuGjDEfVH0pqgY6p/ZnjIkH4t1VjFKusv2k6FnsA1o4PW4OHLCpFlXzaPtTXslTA30N0EFE2ohIEDAWmG9zTarm0PanvJLtgS4is4AVQCcR2ScitxtjCoHJwEJgCzDHGLPJzjqVb9L2p3yJ7cMWlVJKVQ3bj9CVUkpVDQ10pZTyERroSinlIzTQlVLKR2igK6WUj9BAV0opH6GBrpRSPkIDXSmlfIQGulJK+Yj/B6nxugG8OLBDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\David\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 652.058533\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 30.7%\n",
      "Minibatch loss at step 500: 190.591888\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 1000: 115.585449\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 1500: 70.097366\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 2000: 42.511063\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 2500: 25.781542\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 3000: 15.636748\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.3%\n",
      "Test accuracy: 80.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "num_of_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = ((step % num_of_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 0.001}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happening is even with regularization there are too many parameters and the neural network is trained for only a specific set of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay1_train_with_dropout = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(lay1_train_with_dropout, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 512.348877\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 33.0%\n",
      "Minibatch loss at step 500: 36.992397\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1000: 16.571676\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 1500: 10.198216\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 2000: 12.705729\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 2500: 15.009854\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 3000: 10.494297\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.8%\n",
      "Test accuracy: 86.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 0.001}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  global_step = tf.Variable(0)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes1], stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / (num_hidden_nodes1))))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / (num_hidden_nodes2))))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)\n",
    "      + beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3)))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  # Validation\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  \n",
    "  # Testing\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.438914\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 30.4%\n",
      "Minibatch loss at step 500: 1.144194\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 1000: 0.798098\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 1500: 0.605746\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2000: 0.613931\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 2500: 0.599375\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 3000: 0.438260\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.9%\n",
      "Test accuracy: 94.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 0.001}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try using drop out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.345685\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 30.7%\n",
      "Minibatch loss at step 500: 0.591781\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 1000: 0.366509\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 1500: 0.301437\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 2000: 0.313652\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 2500: 0.391163\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 3000: 0.240112\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 3500: 0.234651\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 4000: 0.160594\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 4500: 0.292706\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 5000: 0.274464\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 5500: 0.282315\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 6000: 0.139443\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 6500: 0.158169\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 7000: 0.121206\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 7500: 0.119027\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 8000: 0.138161\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 8500: 0.195306\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 9000: 0.162620\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 9500: 0.100312\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 10000: 0.124947\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 10500: 0.129793\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 11000: 0.169770\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 11500: 0.091446\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 12000: 0.112063\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 12500: 0.086032\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 13000: 0.111323\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 13500: 0.152540\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 14000: 0.149011\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 14500: 0.043729\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 15000: 0.039765\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 15500: 0.166392\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 16000: 0.085499\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 16500: 0.069887\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 17000: 0.058774\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 17500: 0.018261\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 18000: 0.034321\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.0%\n",
      "Test accuracy: 96.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
