{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What I Learned From A Titanic Solution\n",
    "\n",
    "https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to fill in incomplete data\n",
    "\n",
    "When you are trying to fill in incomplete data there are 2 things that you can do\n",
    "\n",
    "1. If it is categorical, you could just take the mode\n",
    "2. If it is quatatative, you could just take the median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out if you columns have NULL values\n",
    "\n",
    "A sneaky good way of figuring out if your columns have null data would just be to do something like\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating bins for your data\n",
    "\n",
    "There is 2 easy ways to create bins for your data. \n",
    "\n",
    "1. cut(): What this will do is cut your variables into bins of equal size\n",
    "\n",
    "    data[\"AgeBin\"] = pd.cut(data[\"Age\"].astype(int), 5)\n",
    "    \n",
    "    \n",
    "2. qcut(): What this will do is cut your variable into bins, where there is the name number of variables in each bin\n",
    "\n",
    "   data[\"AgeBin\"] = pd.qcut(data[\"Age\"].astype(int), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting value counts\n",
    "\n",
    "data[\"ValueCounts\"].value_counts() \n",
    "\n",
    "This will return a object that contains the counts of each unique object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_min = 10\n",
    "title_names = (data1['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n",
    "data1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n",
    "print(data1['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn LabelEncoder()\n",
    "\n",
    "The sklearn library has a label encoder that can be used to encode your categorical variables. Will encode labels with value between 0 and n_classes-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = LabelEncoder()\n",
    "for dataset in data_cleaner:\n",
    "    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n",
    "    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklean ModelSelection()\n",
    "\n",
    "This can be really useful for spliting your data between training and test data.\n",
    "\n",
    "It takes a parameter called test_size, that is an integer between 0 and 1. This integer is the fraction of the data that will be allocated to the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib subplots\n",
    "\n",
    "This with return a figure and axis which can be usefull for graphing multiple things. (numrows, numcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplot(2, 2)\n",
    "\n",
    "axes[0, 0].scatter(x, y)\n",
    "axes[0, 1].plot(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn\n",
    "\n",
    "Seaborn is good if you are doing multi-variable comparison. The FacetGrid function seems ultra usefull."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various kinds of plots that I think are pretty useful\n",
    "\n",
    "__Box Plot__: Good to show the distribution of the data. It will show the median and the box will span the upper and lower quantile range.\n",
    "\n",
    "__Histogram__: These are used for ploting the fequency of some scores appearing in a continuous data set that has been divided into certain classes. Usually, there will be no spaces between the bars signaling the continuity of the data. \n",
    "\n",
    "__Bar Plot__: This is similar to histogram but for categorical variables, this difference can be seen because there are spaces between the bars in this plot. \n",
    "\n",
    "__Point Plot__: Seems like it will just show the median of a value on a point graph.\n",
    "\n",
    "__Scatter Plot__: We all know what a scatter plot is. \n",
    "\n",
    "__Violin Plot__: These are decent at showing the probablility density at various values. \n",
    "\n",
    "__Seaborn FacetGrids__: You can define different variables in your columns and rows and these values will be selected and graphed. \n",
    "\n",
    "__KDE Plot__: This will give you a smoothed probablity density.\n",
    "\n",
    "__Pair Plot__: When you are just comparing two variables. This might just be a generalized term.\n",
    "\n",
    "__Seaborn.distplot__: Combination of a histogram and a kde plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmaps\n",
    "\n",
    "This seems like it super usefull trying to figure out the relationship between all your variables. When doing this be careful that it will only graph values that are numeric. So you can use LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation heatmap of dataset\n",
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        cmap = colormap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
    "        annot_kws={'fontsize':12 }\n",
    "    )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "correlation_heatmap(not_submit_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kinds of ML and loose definition\n",
    "\n",
    "1. Supervised\n",
    "2. Unsupervised\n",
    "3. Reinforced (Hybrid of the last 2)\n",
    "\n",
    "Machine Learning (ML), as the name suggests is teaching the machine how to think and not what to think."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Free Lunch Theorum\n",
    "\n",
    "There is no 'better' machine learning algorithm that works in all scenario, they all work differently in different situations.\n",
    "\n",
    "Although, we are fairly certain that more data always beats better algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative way to search through machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.PassiveAggressiveClassifier(),\n",
    "    linear_model.RidgeClassifierCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    tree.DecisionTreeClassifier(),\n",
    "    tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    \n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    XGBClassifier()    \n",
    "    ]\n",
    "\n",
    "#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n",
    "#note: this is an alternative to train_test_split\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = not_submit_data[\"Survived\"]\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[\"Survived\"], cv  = cv_split)\n",
    "    \n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "    \n",
    "\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(data1[data1_x_bin], data1[Target])\n",
    "    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n",
    "    \n",
    "    row_index+=1\n",
    "\n",
    "    \n",
    "#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n",
    "MLA_compare\n",
    "#MLA_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification vs Regression Algorithms\n",
    "\n",
    "We can generalize that a continuous target variable requires a regression algorithm and a discrete target variable requires a classification algorithm. Since our problem is wether a passenger survived or not, this is a discrete target variable thus we will have to use a classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coinflip (model baselines)\n",
    "\n",
    "If you know nothing about your model and it's classification then if you just random pick a state and guess that for every data point you would have a (1/(number_of_states)) chance of getting that correct. Always compare your model against that.\n",
    "\n",
    "Then compare it when you have more information about your data set. So let's say you have a little information about that dataset, that information being the most popular outcome. If you just guess regardless of the data that this was the outcome, you may want to compare your model against that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Table\n",
    "\n",
    "I think this is similar to a groupby table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "This is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT RFE\n",
    "\n",
    "Recursive feature elimination, not 100% sure what that is but going to figure that out later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter in Python\n",
    "\n",
    "A Counter in python is a closely related to the Dict. It will keep the value you are counting as the key and the count as the value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Extend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add a list of elements to another list easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3]\n",
    "x.extend([4, 5])\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skew in your data is bad\n",
    "\n",
    "Removing outliers is a very good idea\n",
    "\n",
    "Figure out wtf a factorplot is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
